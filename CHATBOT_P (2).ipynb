{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JGTJf3rzgny"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY=''\n",
        "GOOGLE_PALM_API_KEY=''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "def load_secets():\n",
        "    load_dotenv()\n",
        "    env_path = Path(\".\") / \".env\"\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "\n",
        "    open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    google_palm_key = os.getenv(\"GOOGLE_PALM_API_KEY\")\n",
        "\n",
        "    return {\n",
        "        \"OPENAI_API_KEY\": open_ai_key,\n",
        "        \"GOOGLE_PALM_API_KEY\": google_palm_key,\n",
        "    }"
      ],
      "metadata": {
        "id": "FRNm_NZDz72Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Validation(BaseModel):\n",
        "    plan_is_valid: str = Field(\n",
        "        description=\"This field is 'yes' if the plan is feasible, 'no' otherwise\"\n",
        "    )\n",
        "    updated_request: str = Field(description=\"Your update to the plan\")\n",
        "\n",
        "\n",
        "class ValidationTemplate(object):\n",
        "    def __init__(self):\n",
        "        self.system_template = \"\"\"\n",
        "      You are a travel agent who helps users make exciting travel plans.\n",
        "\n",
        "      The user's request will be denoted by four hashtags. Determine if the user's\n",
        "      request is reasonable and achievable within the constraints they set.\n",
        "\n",
        "      A valid request should contain the following:\n",
        "      - A start and end location\n",
        "      - A trip duration that is reasonable given the start and end location\n",
        "      -Trip safer path and also the places to visit in the path from the start location to end location\n",
        "\n",
        "      Any request that contains potentially harmful activities is not valid, regardless of what\n",
        "      other details are provided.\n",
        "\n",
        "      If the request is not valid, set\n",
        "      plan_is_valid = 0 and use your travel expertise to update the request to make it valid,\n",
        "      keeping your revised request shorter than 100 words.\n",
        "\n",
        "      If the request seems reasonable, then set plan_is_valid = 1 and\n",
        "      don't revise the request.\n",
        "\n",
        "      {format_instructions}\n",
        "    \"\"\"\n",
        "\n",
        "        self.human_template = \"\"\"\n",
        "      ####{query}####\n",
        "    \"\"\"\n",
        "\n",
        "        self.parser = PydanticOutputParser(pydantic_object=Validation)\n",
        "\n",
        "        self.system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "            self.system_template,\n",
        "            partial_variables={\n",
        "                \"format_instructions\": self.parser.get_format_instructions()\n",
        "            },\n",
        "        )\n",
        "        self.human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
        "            self.human_template, input_variables=[\"query\"]\n",
        "        )\n",
        "\n",
        "        self.chat_prompt = ChatPromptTemplate.from_messages(\n",
        "            [self.system_message_prompt, self.human_message_prompt]\n",
        "        )"
      ],
      "metadata": {
        "id": "qwsMkZ3_0WWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: CHANGE THE ABOVE FORMAT INSTRUCTIONS TO DISPLAY ANSWER\n",
        "\n",
        "class ValidationTemplate(object):\n",
        "    def __init__(self):\n",
        "        self.system_template = \"\"\"\n",
        "      You are a travel agent who helps users make exciting travel plans.\n",
        "\n",
        "      The user's request will be denoted by four hashtags. Determine if the user's\n",
        "      request is reasonable and achievable within the constraints they set.\n",
        "\n",
        "      A valid request should contain the following:\n",
        "      - A start and end location\n",
        "      - A trip duration that is reasonable given the start and end location\n",
        "      - Some other details, like the user's interests and/or preferred mode of transport\n",
        "\n",
        "      Any request that contains potentially harmful activities is not valid, regardless of what\n",
        "      other details are provided.\n",
        "\n",
        "      If the request is not valid, set\n",
        "      plan_is_valid = 0 and use your travel expertise to update the request to make it valid,\n",
        "      keeping your revised request shorter than 100 words.\n",
        "\n",
        "      If the request seems reasonable, then set plan_is_valid = 1 and\n",
        "      don't revise the request.\n",
        "\n",
        "      {format_instructions}\n",
        "    \"\"\"\n",
        "\n",
        "        self.human_template = \"\"\"\n",
        "      ####{query}####\n",
        "    \"\"\"\n",
        "\n",
        "        self.parser = PydanticOutputParser(pydantic_object=Validation)\n",
        "\n",
        "        self.system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "            self.system_template,\n",
        "            partial_variables={\n",
        "                \"format_instructions\": self.parser.get_format_instructions(\n",
        "                    display_name=\"Validation\",\n",
        "                    show_examples=True,\n",
        "                    max_examples=1,\n",
        "                )\n",
        "            },\n",
        "        )\n",
        "        self.human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
        "            self.human_template, input_variables=[\"query\"]\n",
        "        )\n",
        "\n",
        "        self.chat_prompt = ChatPromptTemplate.from_messages(\n",
        "            [self.system_message_prompt, self.human_message_prompt]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "8C85YRtj-UDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5exBsT80n99",
        "outputId": "dfa60dda-c8c8-4da7-c5b4-0e46d9063067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.32.0-py3-none-any.whl (325 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m286.7/325.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import logging\n",
        "import time\n",
        "# for Palm\n",
        "from langchain.llms import GooglePalm\n",
        "# for OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        open_ai_api_key,\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        debug=True,\n",
        "    ):\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        self._openai_key = open_ai_api_key\n",
        "\n",
        "        self.chat_model = ChatOpenAI(model=model, temperature=temperature, openai_api_key=self._openai_key)\n",
        "        self.validation_prompt = ValidationTemplate()\n",
        "        self.validation_chain = self._set_up_validation_chain(debug)\n",
        "\n",
        "    def _set_up_validation_chain(self, debug=True):\n",
        "\n",
        "        # make validation agent chain\n",
        "        validation_agent = LLMChain(\n",
        "            llm=self.chat_model,\n",
        "            prompt=self.validation_prompt.chat_prompt,\n",
        "            output_parser=self.validation_prompt.parser,\n",
        "            output_key=\"validation_output\",\n",
        "            verbose=debug,\n",
        "        )\n",
        "\n",
        "        # add to sequential chain\n",
        "        overall_chain = SequentialChain(\n",
        "            chains=[validation_agent],\n",
        "            input_variables=[\"query\", \"format_instructions\"],\n",
        "            output_variables=[\"validation_output\"],\n",
        "            verbose=debug,\n",
        "        )\n",
        "\n",
        "        return overall_chain\n",
        "\n",
        "    def validate_travel(self, query):\n",
        "        self.logger.info(\"Validating query\")\n",
        "        t1 = time.time()\n",
        "        self.logger.info(\n",
        "            \"Calling validation (model is {}) on user input\".format(\n",
        "                self.chat_model.model_name\n",
        "            )\n",
        "        )\n",
        "        validation_result = self.validation_chain(\n",
        "            {\n",
        "                \"query\": query,\n",
        "                \"format_instructions\": self.validation_prompt.parser.get_format_instructions(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        validation_test = validation_result[\"validation_output\"].dict()\n",
        "        t2 = time.time()\n",
        "        self.logger.info(\"Time to validate request: {}\".format(round(t2 - t1, 2)))\n",
        "\n",
        "        return validation_test"
      ],
      "metadata": {
        "id": "5fMGk6fz0y6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoYURNVR003R",
        "outputId": "52209fa2-8fe3-43a6-a349-965e3d507993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain_community-0.2.4 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "secrets = load_secets()\n",
        "travel_agent = Agent(open_ai_api_key=\"\", debug=True)\n",
        "\n",
        "\n",
        "query = \"\"\"\n",
        "       I want to do a 5 day roadtrip from Munnar  to Kochi in India.\n",
        "       I want to know the places which i can visit from the start location to end lcoation\n",
        "        \"\"\"\n",
        "\n",
        "travel_agent.validate_travel(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4tk8p800zwy",
        "outputId": "3e249721-67ae-4c53-9c99-b62271a3e4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Validating query\n",
            "INFO:__main__:Calling validation (model is gpt-3.5-turbo) on user input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "      You are a travel agent who helps users make exciting travel plans.\n",
            "\n",
            "      The user's request will be denoted by four hashtags. Determine if the user's\n",
            "      request is reasonable and achievable within the constraints they set.\n",
            "\n",
            "      A valid request should contain the following:\n",
            "      - A start and end location\n",
            "      - A trip duration that is reasonable given the start and end location\n",
            "      -Trip safer path and also the places to visit in the path from the start location to end location\n",
            "\n",
            "      Any request that contains potentially harmful activities is not valid, regardless of what\n",
            "      other details are provided.\n",
            "\n",
            "      If the request is not valid, set\n",
            "      plan_is_valid = 0 and use your travel expertise to update the request to make it valid,\n",
            "      keeping your revised request shorter than 100 words.\n",
            "\n",
            "      If the request seems reasonable, then set plan_is_valid = 1 and\n",
            "      don't revise the request.\n",
            "\n",
            "      The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"plan_is_valid\": {\"description\": \"This field is 'yes' if the plan is feasible, 'no' otherwise\", \"title\": \"Plan Is Valid\", \"type\": \"string\"}, \"updated_request\": {\"description\": \"Your update to the plan\", \"title\": \"Updated Request\", \"type\": \"string\"}}, \"required\": [\"plan_is_valid\", \"updated_request\"]}\n",
            "```\n",
            "    \n",
            "Human: \n",
            "      ####\n",
            "       I want to do a 5 day roadtrip from Munnar  to Kochi in India.\n",
            "       I want to know the places which i can visit from the start location to end lcoation\n",
            "        ####\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Time to validate request: 0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'plan_is_valid': 'yes', 'updated_request': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: why the updated request is not displaying the chain answer\n",
        "\n",
        "# The updated request is not displaying the chain answer because the `validation_test` dictionary does not contain a key called \"updated_request\".\n",
        "\n",
        "# To fix this, you need to modify the `Validation` class to include a field for the updated request. For example:\n",
        "\n",
        "class Validation(BaseModel):\n",
        "    plan_is_valid: str = Field(\n",
        "        description=\"This field is 'yes' if the plan is feasible, 'no' otherwise\"\n",
        "    )\n",
        "    updated_request: str = Field(description=\"Your update to the plan\", default=\"\")\n",
        "\n",
        "# Then, you need to modify the `validate_travel` method to set the `updated_request` field in the `validation_result` dictionary. For example:\n",
        "\n",
        "def validate_travel(self, query):\n",
        "    self.logger.info(\"Validating query\")\n",
        "    t1 = time.time()\n",
        "    self.logger.info(\n",
        "        \"Calling validation (model is {}) on user input\".format(\n",
        "            self.chat_model.model_name\n",
        "        )\n",
        "    )\n",
        "    validation_result = self.validation_chain(\n",
        "        {\n",
        "            \"query\": query,\n",
        "            \"format_instructions\": self.validation_prompt.parser.get_format_instructions(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    validation_test = validation_result[\"validation_output\"].dict()\n",
        "\n",
        "    # Set the updated request field\n",
        "    validation_test[\"updated_request\"] = validation_result[\"validation_output\"].updated_request\n",
        "\n",
        "    t2 = time.time()\n",
        "    self.logger.info(\"Time to validate request: {}\".format(round(t2 - t1, 2)))\n",
        "\n",
        "    return validation_test\n",
        "\n",
        "# Finally, you need to modify the code that prints the updated request to use the new field. For example:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqgGzl4z_DKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQ90WowAGG5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: modify the above to code to get updated_request  in proper answer format.Why answer is not getting displayed\n",
        "h\n",
        "secrets = load_secets()\n",
        "travel_agent = Agent(open_ai_api_key=\"\", debug=True)\n",
        "\n",
        "\n",
        "query = \"\"\"\n",
        "       I want to do a 5 day roadtrip from Munnar  to Kochi in India.\n",
        "       I want to visit the place through mountain views.\n",
        "        \"\"\"\n",
        "\n",
        "validation_test = travel_agent.validate_travel(query)\n",
        "updated_request = validation_test[\"updated_request\"]\n",
        "\n",
        "print(f\"Updated request: {updated_request}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmtj4qPl1Mzg",
        "outputId": "82568b10-5345-43b4-8dc8-fa7484145d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Validating query\n",
            "INFO:__main__:Calling validation (model is gpt-3.5-turbo) on user input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "      You are a travel agent who helps users make exciting travel plans.\n",
            "\n",
            "      The user's request will be denoted by four hashtags. Determine if the user's\n",
            "      request is reasonable and achievable within the constraints they set.\n",
            "\n",
            "      A valid request should contain the following:\n",
            "      - A start and end location\n",
            "      - A trip duration that is reasonable given the start and end location\n",
            "      - Some other details, like the user's interests and/or preferred mode of transport\n",
            "\n",
            "      Any request that contains potentially harmful activities is not valid, regardless of what\n",
            "      other details are provided.\n",
            "\n",
            "      If the request is not valid, set\n",
            "      plan_is_valid = 0 and use your travel expertise to update the request to make it valid,\n",
            "      keeping your revised request shorter than 100 words.\n",
            "\n",
            "      If the request seems reasonable, then set plan_is_valid = 1 and\n",
            "      don't revise the request.\n",
            "\n",
            "      The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"plan_is_valid\": {\"description\": \"This field is 'yes' if the plan is feasible, 'no' otherwise\", \"title\": \"Plan Is Valid\", \"type\": \"string\"}, \"updated_request\": {\"description\": \"Your update to the plan\", \"title\": \"Updated Request\", \"type\": \"string\"}}, \"required\": [\"plan_is_valid\", \"updated_request\"]}\n",
            "```\n",
            "    \n",
            "Human: \n",
            "      ####\n",
            "       I want to do a 5 day roadtrip from Munnar  to Kochi in India.\n",
            "       I want to visit the place through mountain views.\n",
            "        ####\n",
            "    \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Time to validate request: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Updated request: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMXjECZv0esc",
        "outputId": "e816acc1-4514-4ad9-cef9-4b367164da78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.75-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.3 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.75 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z90iKS40ErU",
        "outputId": "4f9ef431-c1ba-460d-c08f-02a7bd386357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GXAqpiPGqwfh",
        "outputId": "e2acbfa4-0e6d-44c4-c8f7-126649bab705"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Timestamp  Main incomer Current R  Main incomer Current Y  \\\n",
              "0  01-08-2023 00:00              173.042465              173.933060   \n",
              "1  01-08-2023 00:15              136.713852              136.000031   \n",
              "2  01-08-2023 00:30              122.184158              120.531586   \n",
              "3  01-08-2023 00:45              147.820511              147.056381   \n",
              "4  01-08-2023 01:00              169.530563              169.660965   \n",
              "\n",
              "   Main incomer Current B  Main incomer VOL RY  Main incomer VOL YB  \\\n",
              "0              182.334091           420.092926           421.757599   \n",
              "1              142.112228           422.147919           423.574371   \n",
              "2              129.242294           423.314911           424.781525   \n",
              "3              154.585083           423.466125           425.228546   \n",
              "4              180.001251           424.328369           426.126343   \n",
              "\n",
              "   Main incomer VOL BR  Main incomer KW  Main incomer PF  \n",
              "0           422.483429              NaN        85.850424  \n",
              "1           424.100830              NaN        82.693070  \n",
              "2           425.750183              NaN        78.835297  \n",
              "3           425.842743              NaN        81.879675  \n",
              "4           426.804993              NaN        85.423458  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d23cd9f1-1684-46d3-b091-97db914e1dd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Main incomer Current R</th>\n",
              "      <th>Main incomer Current Y</th>\n",
              "      <th>Main incomer Current B</th>\n",
              "      <th>Main incomer VOL RY</th>\n",
              "      <th>Main incomer VOL YB</th>\n",
              "      <th>Main incomer VOL BR</th>\n",
              "      <th>Main incomer KW</th>\n",
              "      <th>Main incomer PF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-08-2023 00:00</td>\n",
              "      <td>173.042465</td>\n",
              "      <td>173.933060</td>\n",
              "      <td>182.334091</td>\n",
              "      <td>420.092926</td>\n",
              "      <td>421.757599</td>\n",
              "      <td>422.483429</td>\n",
              "      <td>NaN</td>\n",
              "      <td>85.850424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01-08-2023 00:15</td>\n",
              "      <td>136.713852</td>\n",
              "      <td>136.000031</td>\n",
              "      <td>142.112228</td>\n",
              "      <td>422.147919</td>\n",
              "      <td>423.574371</td>\n",
              "      <td>424.100830</td>\n",
              "      <td>NaN</td>\n",
              "      <td>82.693070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01-08-2023 00:30</td>\n",
              "      <td>122.184158</td>\n",
              "      <td>120.531586</td>\n",
              "      <td>129.242294</td>\n",
              "      <td>423.314911</td>\n",
              "      <td>424.781525</td>\n",
              "      <td>425.750183</td>\n",
              "      <td>NaN</td>\n",
              "      <td>78.835297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01-08-2023 00:45</td>\n",
              "      <td>147.820511</td>\n",
              "      <td>147.056381</td>\n",
              "      <td>154.585083</td>\n",
              "      <td>423.466125</td>\n",
              "      <td>425.228546</td>\n",
              "      <td>425.842743</td>\n",
              "      <td>NaN</td>\n",
              "      <td>81.879675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01-08-2023 01:00</td>\n",
              "      <td>169.530563</td>\n",
              "      <td>169.660965</td>\n",
              "      <td>180.001251</td>\n",
              "      <td>424.328369</td>\n",
              "      <td>426.126343</td>\n",
              "      <td>426.804993</td>\n",
              "      <td>NaN</td>\n",
              "      <td>85.423458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d23cd9f1-1684-46d3-b091-97db914e1dd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d23cd9f1-1684-46d3-b091-97db914e1dd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d23cd9f1-1684-46d3-b091-97db914e1dd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72d9b42e-e8cf-4eef-9570-739de010ba24\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72d9b42e-e8cf-4eef-9570-739de010ba24')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72d9b42e-e8cf-4eef-9570-739de010ba24 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 15308,\n  \"fields\": [\n    {\n      \"column\": \"Timestamp\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11674,\n        \"samples\": [\n          \"27-12-2023 08:00\",\n          \"01-10-2023 16:30\",\n          \"23-08-2023 11:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer Current R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 59.16443587875063,\n        \"min\": 8.281964302,\n        \"max\": 307.4018555,\n        \"num_unique_values\": 11242,\n        \"samples\": [\n          178.8894196,\n          226.3275757,\n          128.0601807\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer Current Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 59.92999599117209,\n        \"min\": 7.32479763,\n        \"max\": 290.1878052,\n        \"num_unique_values\": 11142,\n        \"samples\": [\n          213.800766,\n          177.7924042,\n          125.1650925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer Current B\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.18452832995673,\n        \"min\": 7.951947689,\n        \"max\": 345.9102478,\n        \"num_unique_values\": 11356,\n        \"samples\": [\n          126.037178,\n          150.9346161,\n          142.3262939\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer VOL RY\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.664422299455042,\n        \"min\": 340.7221375,\n        \"max\": 432.7937317,\n        \"num_unique_values\": 11537,\n        \"samples\": [\n          410.4059143,\n          410.437561,\n          424.5825806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer VOL YB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.754299030330089,\n        \"min\": 340.7703247,\n        \"max\": 435.0676575,\n        \"num_unique_values\": 11525,\n        \"samples\": [\n          419.7196655,\n          423.8085632,\n          412.1896667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer VOL BR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.748441554027641,\n        \"min\": 341.1858826,\n        \"max\": 436.4848633,\n        \"num_unique_values\": 11563,\n        \"samples\": [\n          425.6181641,\n          413.1640625,\n          428.8592834\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer KW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Main incomer PF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.137755009785956,\n        \"min\": 50.44777989,\n        \"max\": 95.2498138,\n        \"num_unique_values\": 11648,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Set Timestamp as the index\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Handle duplicate timestamps by taking the mean value\n",
        "data = data.groupby(data.index).mean()\n",
        "\n",
        "# Ensure the data has a defined frequency\n",
        "data = data.asfreq('15T')\n",
        "\n",
        "# Select the \"Main incomer Current R\" column and handle missing values\n",
        "data_series = data['Main incomer Current R'].fillna(method='ffill')\n",
        "\n",
        "# Filter data from August to December\n",
        "data_series_aug_dec = data_series['2023-08-01':'2023-12-31']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data_series_scaled = scaler.fit_transform(data_series_aug_dec.values.reshape(-1, 1))\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data)-seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 60  # Using the past 60 time steps to predict the next one\n",
        "X, y = create_sequences(data_series_scaled, seq_length)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.from_numpy(X).float()\n",
        "y_train = torch.from_numpy(y).float()\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
        "        self.hidden_cell = (Variable(torch.zeros(1, 1, self.hidden_layer_size)),\n",
        "                            Variable(torch.zeros(1, 1, self.hidden_layer_size)))\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
        "        predictions = self.linear(lstm_out[:, -1])\n",
        "        return predictions\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = LSTMModel()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(X_train)):\n",
        "        model.hidden_cell = (Variable(torch.zeros(1, 1, model.hidden_layer_size)),\n",
        "                             Variable(torch.zeros(1, 1, model.hidden_layer_size)))\n",
        "\n",
        "        y_pred = model(X_train[i].unsqueeze(0))\n",
        "\n",
        "        single_loss = loss_function(y_pred, y_train[i].unsqueeze(0))\n",
        "        optimizer.zero_grad()\n",
        "        single_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch: {epoch:3} loss: {single_loss.item():10.8f}')\n",
        "\n",
        "# Prepare forecast for January\n",
        "model.eval()\n",
        "test_inputs = data_series_scaled[-seq_length:].tolist()\n",
        "\n",
        "january_forecast = []\n",
        "for i in range(31 * 24 * 4):  # Predicting 31 days with 15-minute intervals\n",
        "    seq = torch.FloatTensor(test_inputs[-seq_length:]).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        model.hidden_cell = (Variable(torch.zeros(1, 1, model.hidden_layer_size)),\n",
        "                             Variable(torch.zeros(1, 1, model.hidden_layer_size)))\n",
        "        january_forecast.append(model(seq).item())\n",
        "        test_inputs.append([model(seq).item()])\n",
        "\n",
        "# Inverse transform the forecasted data\n",
        "january_forecast = scaler.inverse_transform(np.array(january_forecast).reshape(-1, 1))\n",
        "\n",
        "# Create date range for the forecast\n",
        "forecast_index = pd.date_range(start=data_series_aug_dec.index[-1], periods=len(january_forecast)+1, freq='15T')[1:]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_series, label='Original Data')\n",
        "plt.plot(forecast_index, january_forecast, label='January Forecast', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Current R')\n",
        "plt.title('January Forecast using LSTM')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Combine original data and forecasted data into a DataFrame\n",
        "forecast_data = pd.DataFrame({\n",
        "    'Original Data': data_series,\n",
        "    'January Forecast': np.concatenate((np.full(len(data_series)-len(january_forecast), np.nan), january_forecast.flatten())),\n",
        "}, index=data_series.index)\n",
        "\n",
        "# Export the combined data to an Excel file\n",
        "excel_filename = 'forecasted_data.xlsx'\n",
        "forecast_data.to_excel(excel_filename)\n",
        "\n",
        "print(f'Forecasted data saved to {excel_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "PkEycFQyrC84",
        "outputId": "42133ce1-86a7-41ca-db8f-0e47d5f0d047"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 loss: 0.00001662\n",
            "epoch:  10 loss: 0.00000418\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0422886beac6>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0msingle_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0msingle_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HuoVV4U5HYLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Set Timestamp as the index\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Handle duplicate timestamps by taking the mean value\n",
        "data = data.groupby(data.index).mean()\n",
        "\n",
        "# Ensure the data has a defined frequency\n",
        "data = data.asfreq('15T')\n",
        "\n",
        "# Select the \"Main incomer Current R\" column and handle missing values\n",
        "data_series = data['Main incomer Current R'].fillna(method='ffill')\n",
        "\n",
        "# Filter data from August to December\n",
        "data_series_aug_dec = data_series['2023-08-01':'2023-12-31']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data_series_scaled = scaler.fit_transform(data_series_aug_dec.values.reshape(-1, 1))\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data)-seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 60  # Using the past 60 time steps to predict the next one\n",
        "X, y = create_sequences(data_series_scaled, seq_length)\n",
        "\n",
        "# Reshape X to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Define the LSTM model using Keras Sequential API\n",
        "model = Sequential([\n",
        "    LSTM(units=50, input_shape=(seq_length, 1)),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Prepare forecast for January\n",
        "test_inputs = data_series_scaled[-seq_length:].tolist()\n",
        "test_inputs = np.array(test_inputs).reshape(1, seq_length, 1)\n",
        "\n",
        "january_forecast = []\n",
        "for i in range(31 * 24 * 4):  # Predicting 31 days with 15-minute intervals\n",
        "    prediction = model.predict(test_inputs)\n",
        "    january_forecast.append(prediction[0, 0])\n",
        "    test_inputs = np.roll(test_inputs, -1)\n",
        "    test_inputs[0, -1, 0] = prediction\n",
        "\n",
        "# Inverse transform the forecasted data\n",
        "january_forecast = scaler.inverse_transform(np.array(january_forecast).reshape(-1, 1))\n",
        "\n",
        "# Create date range for the forecast\n",
        "forecast_index = pd.date_range(start=data_series_aug_dec.index[-1], periods=len(january_forecast)+1, freq='15T')[1:]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_series, label='Original Data')\n",
        "plt.plot(forecast_index, january_forecast, label='January Forecast', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Current R')\n",
        "plt.title('January Forecast using LSTM (TensorFlow/Keras)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Combine original data and forecasted data into a DataFrame\n",
        "forecast_data = pd.DataFrame({\n",
        "    'Original Data': data_series,\n",
        "    'January Forecast': np.concatenate((np.full(len(data_series)-len(january_forecast), np.nan), january_forecast.flatten())),\n",
        "}, index=data_series.index)\n",
        "\n",
        "# Export the combined data to an Excel file\n",
        "excel_filename = 'forecasted_data_tensorflow.xlsx'\n",
        "forecast_data.to_excel(excel_filename)\n",
        "\n",
        "print(f'Forecasted data saved to {excel_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ur15tUS2NCA9",
        "outputId": "161c361b-379b-4a1b-c989-f5f7ddca2bad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 50)                10400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10451 (40.82 KB)\n",
            "Trainable params: 10451 (40.82 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "229/229 [==============================] - 10s 31ms/step - loss: 0.0316\n",
            "Epoch 2/100\n",
            "229/229 [==============================] - 8s 36ms/step - loss: 0.0238\n",
            "Epoch 3/100\n",
            "229/229 [==============================] - 7s 29ms/step - loss: 0.0229\n",
            "Epoch 4/100\n",
            "229/229 [==============================] - 9s 39ms/step - loss: 0.0224\n",
            "Epoch 5/100\n",
            "229/229 [==============================] - 7s 32ms/step - loss: 0.0223\n",
            "Epoch 6/100\n",
            "229/229 [==============================] - 8s 34ms/step - loss: 0.0222\n",
            "Epoch 7/100\n",
            "229/229 [==============================] - 7s 33ms/step - loss: 0.0223\n",
            "Epoch 8/100\n",
            "229/229 [==============================] - 7s 32ms/step - loss: 0.0222\n",
            "Epoch 9/100\n",
            "229/229 [==============================] - 8s 34ms/step - loss: 0.0222\n",
            "Epoch 10/100\n",
            "229/229 [==============================] - 7s 31ms/step - loss: 0.0221\n",
            "Epoch 11/100\n",
            "229/229 [==============================] - 9s 41ms/step - loss: 0.0221\n",
            "Epoch 12/100\n",
            "229/229 [==============================] - 7s 32ms/step - loss: 0.0221\n",
            "Epoch 13/100\n",
            "229/229 [==============================] - 8s 36ms/step - loss: 0.0221\n",
            "Epoch 14/100\n",
            "229/229 [==============================] - 7s 31ms/step - loss: 0.0220\n",
            "Epoch 15/100\n",
            "229/229 [==============================] - 8s 34ms/step - loss: 0.0220\n",
            "Epoch 16/100\n",
            "229/229 [==============================] - 8s 34ms/step - loss: 0.0219\n",
            "Epoch 17/100\n",
            "229/229 [==============================] - 7s 32ms/step - loss: 0.0218\n",
            "Epoch 18/100\n",
            "229/229 [==============================] - 8s 35ms/step - loss: 0.0218\n",
            "Epoch 19/100\n",
            "229/229 [==============================] - 7s 31ms/step - loss: 0.0216\n",
            "Epoch 20/100\n",
            "229/229 [==============================] - 8s 36ms/step - loss: 0.0216\n",
            "Epoch 21/100\n",
            "229/229 [==============================] - 7s 30ms/step - loss: 0.0216\n",
            "Epoch 22/100\n",
            "229/229 [==============================] - 10s 42ms/step - loss: 0.0216\n",
            "Epoch 23/100\n",
            "229/229 [==============================] - 9s 38ms/step - loss: 0.0215\n",
            "Epoch 24/100\n",
            "229/229 [==============================] - 8s 36ms/step - loss: 0.0214\n",
            "Epoch 25/100\n",
            "229/229 [==============================] - 9s 39ms/step - loss: 0.0214\n",
            "Epoch 26/100\n",
            "229/229 [==============================] - 7s 30ms/step - loss: 0.0213\n",
            "Epoch 27/100\n",
            "229/229 [==============================] - 9s 38ms/step - loss: 0.0214\n",
            "Epoch 28/100\n",
            " 49/229 [=====>........................] - ETA: 5s - loss: 0.0211"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-76f4073064f8>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Prepare forecast for January\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location, vehicle, condition, fuel, companion, men):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = (f\"Given the following details:\\n\"\n",
        "              f\"Vehicle: {vehicle}\\n\"\n",
        "              f\"Condition: {condition}\\n\"\n",
        "              f\"Fuel: {fuel}\\n\"\n",
        "              f\"Companion: {companion}\\n\"\n",
        "              f\"Men: {men}\\n\\n\"\n",
        "              f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\")\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"Enter your request: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use 'HEY TAKE ME FROM [start location] TO [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    vehicle = input(\"Which vehicle are you driving? \")\n",
        "    condition = input(\"How's the condition of the vehicle? Well serviced? \")\n",
        "    fuel = input(\"How filled is your fuel tank? \")\n",
        "    companion = input(\"Who is accompanying you or are you alone? \")\n",
        "    men = input(\"Are there any men with you? \")\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location, vehicle, condition, fuel, companion, men)\n",
        "    print(\"Route Description:\", route_description)\n",
        "\n",
        "    while True:\n",
        "        satisfied = input(\"Are you satisfied with this route? (yes/no): \").strip().lower()\n",
        "        if satisfied == 'yes':\n",
        "            print(\"Great! Have a safe journey!\")\n",
        "            break\n",
        "        elif satisfied == 'no':\n",
        "            print(\"Sorry, but that's the only route available.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Please answer 'yes' or 'no'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "NE2vxADTc8M_",
        "outputId": "d14977b3-75f6-4ff8-eaf0-d16ab34b9748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b9d696ee68fc>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input in English or Hindi\n",
        "    pattern = re.compile(r\"(FROM|से)\\s+(.+?)\\s+(TO|को)\\s+(.+)\", re.IGNORECASE | re.UNICODE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(2).strip()\n",
        "        end_location = match.group(4).strip()\n",
        "        return start_location, end_location\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def get_user_input(prompt, language):\n",
        "    if language == \"en\":\n",
        "        return input(prompt).strip()\n",
        "    elif language == \"hi\":\n",
        "        return input(prompt.encode('utf-8').decode('utf-8')).strip()  # Use Unicode for Hindi input handling\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported language. Please choose 'en' for English or 'hi' for Hindi.\")\n",
        "\n",
        "def generate_route_description(start_location, end_location, vehicle, condition, fuel, companion, men):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = (f\"Given the following details:\\n\"\n",
        "              f\"Vehicle: {vehicle}\\n\"\n",
        "              f\"Condition: {condition}\\n\"\n",
        "              f\"Fuel: {fuel}\\n\"\n",
        "              f\"Companion: {companion}\\n\"\n",
        "              f\"Men: {men}\\n\\n\"\n",
        "              f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\")\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        language=\"hi\",  # Specify language as Hindi for responses\n",
        "        max_tokens=150,  # Adjust as needed\n",
        "        stop=[\".\"]  # Stop generating after a period to form coherent sentences\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = get_user_input(\"Enter your request (अपनी अनुरोध दर्ज करें): \", \"hi\")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting (निकासी)')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use 'HEY TAKE ME FROM [start location] TO [end location]' (अमान्य इनपुट प्रारूप। कृपया 'हेय टेक मे फ्रॉम [शुरूआत स्थान] टू [अंत स्थान]' का उपयोग करें।)\")\n",
        "        continue\n",
        "\n",
        "    vehicle = get_user_input(\"Which vehicle are you driving? (आप किस वाहन का उपयोग कर रहे हैं?): \", \"hi\")\n",
        "    condition = get_user_input(\"How's the condition of the vehicle? Well serviced? (वाहन की हालत कैसी है? अच्छी सेवा की गई है?): \", \"hi\")\n",
        "    fuel = get_user_input(\"How filled is your fuel tank? (आपके ईंधन टैंक में कितनी भराई हुई है?): \", \"hi\")\n",
        "    companion = get_user_input(\"Who is accompanying you or are you alone? (आपके साथ कौन है या क्या आप अकेले हैं?): \", \"hi\")\n",
        "    men = get_user_input(\"Are there any men with you? (क्या आपके साथ कोई आदमी हैं?): \", \"hi\")\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location, vehicle, condition, fuel, companion, men)\n",
        "    print(\"Route Description (मार्ग विवरण):\", route_description)\n",
        "\n",
        "    while True:\n",
        "        satisfied = get_user_input(\"Are you satisfied with this route? (yes/no) (क्या आप इस मार्ग से संतुष्ट हैं?): \", \"hi\")\n",
        "        if satisfied == 'yes':\n",
        "            print(\"Great! Have a safe journey! (बहुत बढ़िया! सुरक्षित यात्रा करें!)\")\n",
        "            break\n",
        "        elif satisfied == 'no':\n",
        "            print(\"Sorry, but that's the only route available. (माफ़ कीजिए, लेकिन यही मार्ग उपलब्ध है।)\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Please answer 'yes' or 'no'. (कृपया 'हां' या 'नहीं' का उत्तर दें।)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "_EZA0bhte22f",
        "outputId": "6fe02a1d-af53-4f90-fc26-197d26fb1835"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter your request (अपनी अनुरोध दर्ज करें): Take me from kannur to kozhikode\n",
            "Which vehicle are you driving? (आप किस वाहन का उपयोग कर रहे हैं?): BMW\n",
            "How's the condition of the vehicle? Well serviced? (वाहन की हालत कैसी है? अच्छी सेवा की गई है?): GOOD\n",
            "How filled is your fuel tank? (आपके ईंधन टैंक में कितनी भराई हुई है?): FULL\n",
            "Who is accompanying you or are you alone? (आपके साथ कौन है या क्या आप अकेले हैं?): ALONE\n",
            "Are there any men with you? (क्या आपके साथ कोई आदमी हैं?): YES\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "Unrecognized request argument supplied: language",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0c986ef1f6c5>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mmen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Are there any men with you? (क्या आपके साथ कोई आदमी हैं?): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mroute_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_route_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvehicle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description (मार्ग विवरण):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0c986ef1f6c5>\u001b[0m in \u001b[0;36mgenerate_route_description\u001b[0;34m(start_location, end_location, vehicle, condition, fuel, companion, men)\u001b[0m\n\u001b[1;32m     36\u001b[0m               f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\")\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Unrecognized request argument supplied: language"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] =\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = user_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Iterate over words to find locations\n",
        "    for i in range(len(words)):\n",
        "        if words[i] in direction_keywords:\n",
        "            if i + 1 < len(words):\n",
        "                if start_location is None:\n",
        "                    start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                elif end_location is None:\n",
        "                    end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KnHwZv5kl4sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVxK0o_PmTlL",
        "outputId": "7a24b544-fd7b-44c0-b1a0-3fb743e3f4da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request: take me from kannur to kozhikode\n",
            "Route Description: I'll provide you with a rough driving route from Me to Kannur. Please note that distances and landmarks may vary based on your exact location and road conditions.\n",
            "\n",
            "1. Start from Me and head south on the main road.\n",
            "2. After driving for approximately 5 kilometers, you will reach a major intersection. Turn left (east) at this intersection.\n",
            "3. Continue on this road for another 10 kilometers until you reach a T-junction. Turn right (south) at the junction.\n",
            "4. Stay on this road for about 20 kilometers until you reach a roundabout. Take the second exit to stay on the main road.\n",
            "5. Drive for another 15 kilometers until you reach a major city. Drive through the city following signs for Kannur.\n",
            "6. After exiting the city, continue on the main road for approximately 50 kilometers. You'll pass through several towns and villages along the way.\n",
            "7. Once you reach Kannur city limits, follow signs to reach your specific destination within Kannur.\n",
            "\n",
            "Please note that these directions are approximate and may vary based on current traffic conditions, road closures, and detours. I recommend using a GPS navigation system or a map for real-time updates and the most accurate route information.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: Provide path from kannur to kozhikode\n",
            "Route Description: Sure! The driving distance between Kannur and Kozhikode is approximately 91 kilometers, and the estimated driving time is around 2 hours, depending on traffic conditions. Here is a detailed route with major turns, landmarks, and distances:\n",
            "\n",
            "1. Start from Kannur and head south on NH 66 (National Highway 66).\n",
            "2. Drive for about 25 kilometers until you reach Thalassery. Look out for landmarks such as Muzhappilangad Beach and Mahe River Bridge along the way.\n",
            "3. Continue on NH 66 and drive for another 25 kilometers until you reach Kuthuparamba.\n",
            "4. At Kuthuparamba, take a slight left onto NH 766.\n",
            "5. Drive for about 16 kilometers until you reach Vadakara. Look out for landmarks such as Sand Banks and the Vadakara Beach.\n",
            "6. Continue on NH 766 and drive for another 25 kilometers until you reach Kozhikode.\n",
            "7. Upon reaching Kozhikode, you can navigate towards your specific destination within the city using GPS or local directions.\n",
            "\n",
            "Remember to follow traffic rules, keep an eye on road signs, and drive safely throughout your journey. Have a safe and pleasant trip from Kannur to Kozhikode!\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: Provide the most optimal path from Kannur to Kozhikode\n",
            "Route Description: Certainly! Here is a detailed driving route from Kannur to Kozhikode:\n",
            "\n",
            "1. Start at Kannur and head south on the NH 66 (also known as NH 66B) towards Kozhikode.\n",
            "2. Drive for approximately 40 km until you reach Thalassery. You will pass through towns like Pappinisseri and Kuthuparamba along the way.\n",
            "3. Continue on NH 66 and drive for another 35 km until you reach Mahe. You will pass by towns like Chala, Chirakkal, and Chalad.\n",
            "4. After Mahe, drive for around 35 km on NH 66 to reach Vadakara. You will pass through towns like Edacheri and Mahe.\n",
            "5. From Vadakara, continue on NH 66 for another 48 km to reach Kozhikode. You will pass through towns like Ulliyeri and Ullal before reaching Kozhikode.\n",
            "\n",
            "Important landmarks along the way:\n",
            "- Thalassery: Known for its cultural heritage and historic buildings.\n",
            "- Mahe: A small town with a strong French influence and beautiful beaches.\n",
            "- Vadakara: A historical town with temples and churches dating back centuries.\n",
            "- Kozhikode: A major city known for its scenic beauty, historic sites, and bustling markets.\n",
            "\n",
            "Total distance: Approximately 158 km\n",
            "Estimated driving time: Around 3-4 hours, depending on traffic conditions.\n",
            "\n",
            "Please note that road conditions may vary, so it's advisable to check for any updates or alternate routes before starting your journey. Drive safely!\n",
            "Enter 'text' for text input or 'audio' for audio input: eXIT\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04r5mWJomWTB",
        "outputId": "276a911d-d273-4fdb-ec43-2b7431f2f5ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=5145b15346046c608c65aa7d1be99752e9ecd36e3a72e97360a8ac7f36753aec\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOboHF3kkJca",
        "outputId": "1f57cb11-2356-48df-cdb2-44bd04f64ae2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m883.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade googletrans==3.1.0a0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAxKs7HZfVhd",
        "outputId": "1df18a53-ef1c-4bca-d567-c230e5151916"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.6.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "id": "rfuZr3i_e4jG",
        "outputId": "983228b5-8f92-4826-c627-883110a4016e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.6.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=ce765be72e54947d4916a6d3214a1897261141e32144b85fd5a515ab32112999\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "5051296f09a942d09c59ae85f019e0af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P68bWpqjeauJ",
        "outputId": "40517780-f98b-4044-8562-fc71359baef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.34.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.9.0\n",
            "    Uninstalling h11-0.9.0:\n",
            "      Successfully uninstalled h11-0.9.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 0.9.1\n",
            "    Uninstalling httpcore-0.9.1:\n",
            "      Successfully uninstalled httpcore-0.9.1\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.13.3\n",
            "    Uninstalling httpx-0.13.3:\n",
            "      Successfully uninstalled httpx-0.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "googletrans 3.1.0a0 requires httpx==0.13.3, but you have httpx 0.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Set Timestamp as the index\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Handle duplicate timestamps by taking the mean value\n",
        "data = data.groupby(data.index).mean()\n",
        "\n",
        "# Ensure the data has a defined frequency\n",
        "data = data.asfreq('15T')\n",
        "\n",
        "# Select the \"Main incomer Current R\" column and handle missing values\n",
        "data_series = data['Main incomer Current R'].fillna(method='ffill')\n",
        "\n",
        "# Filter data from August to December\n",
        "data_series_aug_dec = data_series['2023-08-01':'2023-12-31']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data_series_scaled = scaler.fit_transform(data_series_aug_dec.values.reshape(-1, 1))\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i + seq_length]\n",
        "        y = data[i + seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 60  # Sequence length to use for LSTM\n",
        "X, y = create_sequences(data_series_scaled, seq_length)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_val = X[:train_size], X[train_size:]\n",
        "y_train, y_val = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=False, input_shape=(seq_length, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Forecast the data for January\n",
        "test_inputs = data_series_scaled[-seq_length:].tolist()\n",
        "january_forecast = []\n",
        "\n",
        "for i in range(31 * 24 * 4):  # Predicting 31 days with 15-minute intervals\n",
        "    seq = np.array(test_inputs[-seq_length:]).reshape(1, seq_length, 1)\n",
        "    pred = model.predict(seq)\n",
        "    january_forecast.append(pred[0][0])\n",
        "    test_inputs.append(pred[0][0])\n",
        "\n",
        "# Inverse transform the forecasted data\n",
        "january_forecast = scaler.inverse_transform(np.array(january_forecast).reshape(-1, 1))\n",
        "\n",
        "# Create date range for the forecast\n",
        "forecast_index = pd.date_range(start=data_series_aug_dec.index[-1], periods=len(january_forecast) + 1, freq='15T')[1:]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_series, label='Original Data')\n",
        "plt.plot(forecast_index, january_forecast, label='January Forecast', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Current R')\n",
        "plt.title('January Forecast using LSTM')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RzdamAsp5hzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Set Timestamp as the index\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Handle duplicate timestamps by taking the mean value\n",
        "data = data.groupby(data.index).mean()\n",
        "\n",
        "# Ensure the data has a defined frequency\n",
        "data = data.asfreq('15T')\n",
        "\n",
        "# Select the \"Main incomer Current R\" column and handle missing values\n",
        "data_series = data['Main incomer Current R'].fillna(method='ffill')\n",
        "\n",
        "# Filter data from August to December\n",
        "data_series_aug_dec = data_series['2023-08-01':'2023-12-31']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data_series_scaled = scaler.fit_transform(data_series_aug_dec.values.reshape(-1, 1))\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data)-seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 30  # Reduced sequence length for faster runtime\n",
        "X, y = create_sequences(data_series_scaled, seq_length)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_val = X[:train_size], X[train_size:]\n",
        "y_train, y_val = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=False, input_shape=(seq_length, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Forecast the data for January\n",
        "test_inputs = data_series_scaled[-seq_length:].tolist()\n",
        "test_inputs = [list(x) for x in test_inputs]  # Ensure test_inputs is a list of lists\n",
        "january_forecast = []\n",
        "\n",
        "for i in range(31 * 24 * 4):  # Predicting 31 days with 15-minute intervals\n",
        "    seq = np.array(test_inputs[-seq_length:]).reshape(1, seq_length, 1)\n",
        "    pred = model.predict(seq)\n",
        "    january_forecast.append(pred[0][0])\n",
        "    test_inputs.append([pred[0][0]])\n",
        "\n",
        "# Inverse transform the forecasted data\n",
        "january_forecast = scaler.inverse_transform(np.array(january_forecast).reshape(-1, 1))\n",
        "\n",
        "# Create date range for the forecast\n",
        "forecast_index = pd.date_range(start=data_series_aug_dec.index[-1], periods=len(january_forecast)+1, freq='15T')[1:]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_series, label='Original Data')\n",
        "plt.plot(forecast_index, january_forecast, label='January Forecast', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Current R')\n",
        "plt.title('January Forecast using LSTM')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WQtEqqsoyxG-",
        "outputId": "bcc2a360-c3f6-42a4-ac2f-00a1105fddc8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "184/184 [==============================] - 11s 42ms/step - loss: 0.0407 - val_loss: 0.0288\n",
            "Epoch 2/100\n",
            "184/184 [==============================] - 4s 23ms/step - loss: 0.0242 - val_loss: 0.0273\n",
            "Epoch 3/100\n",
            "184/184 [==============================] - 4s 20ms/step - loss: 0.0229 - val_loss: 0.0263\n",
            "Epoch 4/100\n",
            "184/184 [==============================] - 5s 27ms/step - loss: 0.0223 - val_loss: 0.0260\n",
            "Epoch 5/100\n",
            "184/184 [==============================] - 4s 21ms/step - loss: 0.0220 - val_loss: 0.0262\n",
            "Epoch 6/100\n",
            "184/184 [==============================] - 3s 19ms/step - loss: 0.0218 - val_loss: 0.0254\n",
            "Epoch 7/100\n",
            "184/184 [==============================] - 4s 21ms/step - loss: 0.0218 - val_loss: 0.0249\n",
            "Epoch 8/100\n",
            "184/184 [==============================] - 6s 34ms/step - loss: 0.0216 - val_loss: 0.0255\n",
            "Epoch 9/100\n",
            "184/184 [==============================] - 4s 20ms/step - loss: 0.0216 - val_loss: 0.0249\n",
            "Epoch 10/100\n",
            "184/184 [==============================] - 3s 18ms/step - loss: 0.0216 - val_loss: 0.0257\n",
            "Epoch 11/100\n",
            "184/184 [==============================] - 4s 24ms/step - loss: 0.0216 - val_loss: 0.0246\n",
            "Epoch 12/100\n",
            "184/184 [==============================] - 5s 25ms/step - loss: 0.0216 - val_loss: 0.0248\n",
            "Epoch 13/100\n",
            "184/184 [==============================] - 4s 19ms/step - loss: 0.0216 - val_loss: 0.0245\n",
            "Epoch 14/100\n",
            "184/184 [==============================] - 4s 20ms/step - loss: 0.0215 - val_loss: 0.0246\n",
            "Epoch 15/100\n",
            "184/184 [==============================] - 5s 25ms/step - loss: 0.0215 - val_loss: 0.0246\n",
            "Epoch 16/100\n",
            "184/184 [==============================] - 4s 23ms/step - loss: 0.0216 - val_loss: 0.0248\n",
            "Epoch 17/100\n",
            "184/184 [==============================] - 3s 19ms/step - loss: 0.0215 - val_loss: 0.0249\n",
            "Epoch 18/100\n",
            "184/184 [==============================] - 4s 20ms/step - loss: 0.0215 - val_loss: 0.0247\n",
            "Epoch 19/100\n",
            "184/184 [==============================] - 5s 29ms/step - loss: 0.0215 - val_loss: 0.0245\n",
            "Epoch 20/100\n",
            "184/184 [==============================] - 3s 19ms/step - loss: 0.0215 - val_loss: 0.0245\n",
            "Epoch 21/100\n",
            "184/184 [==============================] - 3s 19ms/step - loss: 0.0214 - val_loss: 0.0247\n",
            "Epoch 22/100\n",
            "184/184 [==============================] - 4s 23ms/step - loss: 0.0214 - val_loss: 0.0246\n",
            "Epoch 23/100\n",
            "184/184 [==============================] - 5s 26ms/step - loss: 0.0214 - val_loss: 0.0248\n",
            "Epoch 24/100\n",
            "184/184 [==============================] - 3s 19ms/step - loss: 0.0214 - val_loss: 0.0248\n",
            "Epoch 25/100\n",
            "184/184 [==============================] - 4s 22ms/step - loss: 0.0213 - val_loss: 0.0250\n",
            "Epoch 26/100\n",
            "184/184 [==============================] - 5s 26ms/step - loss: 0.0213 - val_loss: 0.0245\n",
            "Epoch 27/100\n",
            "184/184 [==============================] - 4s 19ms/step - loss: 0.0213 - val_loss: 0.0246\n",
            "Epoch 28/100\n",
            "184/184 [==============================] - 4s 22ms/step - loss: 0.0211 - val_loss: 0.0247\n",
            "Epoch 29/100\n",
            "184/184 [==============================] - 4s 21ms/step - loss: 0.0211 - val_loss: 0.0258\n",
            "1/1 [==============================] - 0s 415ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4cb0216c4a75>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predicting 31 days with 15-minute intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mjanuary_forecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtest_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    743\u001b[0m             self._flat_output_types)\n\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3422\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3423\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Set Timestamp as the index\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Handle duplicate timestamps by taking the mean value\n",
        "data = data.groupby(data.index).mean()\n",
        "\n",
        "# Ensure the data has a defined frequency\n",
        "data = data.asfreq('15T')\n",
        "\n",
        "# Select the \"Main incomer Current R\" column and handle missing values\n",
        "data_series = data['Main incomer Current R'].fillna(method='ffill')\n",
        "\n",
        "# Filter data from August to December\n",
        "data_series_aug_dec = data_series['2023-08-01':'2023-12-31']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "data_series_scaled = scaler.fit_transform(data_series_aug_dec.values.reshape(-1, 1))\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i + seq_length]\n",
        "        y = data[i + seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 60  # Sequence length to use for LSTM\n",
        "X, y = create_sequences(data_series_scaled, seq_length)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_val = X[:train_size], X[train_size:]\n",
        "y_train, y_val = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=False, input_shape=(seq_length, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Forecast the data for January\n",
        "test_inputs = data_series_scaled[-seq_length:].tolist()\n",
        "january_forecast = []\n",
        "\n",
        "for i in range(31 * 24 * 4):  # Predicting 31 days with 15-minute intervals\n",
        "    seq = np.array(test_inputs[-seq_length:]).reshape(1, seq_length, 1)\n",
        "    pred = model.predict(seq)\n",
        "    january_forecast.append(pred[0][0])\n",
        "    test_inputs.append(pred[0][0])\n",
        "\n",
        "# Inverse transform the forecasted data\n",
        "january_forecast = scaler.inverse_transform(np.array(january_forecast).reshape(-1, 1))\n",
        "\n",
        "# Create date range for the forecast\n",
        "forecast_index = pd.date_range(start=data_series_aug_dec.index[-1], periods=len(january_forecast) + 1, freq='15T')[1:]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_series, label='Original Data')\n",
        "plt.plot(forecast_index, january_forecast, label='January Forecast', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Current R')\n",
        "plt.title('January Forecast using LSTM')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WBbuwbalNGNU",
        "outputId": "689e546d-f9bc-44e4-aac9-7c171d17b45c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "183/183 [==============================] - 7s 14ms/step - loss: 0.0318 - val_loss: 0.0278\n",
            "Epoch 2/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0233 - val_loss: 0.0266\n",
            "Epoch 3/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0225 - val_loss: 0.0260\n",
            "Epoch 4/100\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.0220 - val_loss: 0.0255\n",
            "Epoch 5/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0218 - val_loss: 0.0256\n",
            "Epoch 6/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0217 - val_loss: 0.0253\n",
            "Epoch 7/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0215 - val_loss: 0.0251\n",
            "Epoch 8/100\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.0215 - val_loss: 0.0253\n",
            "Epoch 9/100\n",
            "183/183 [==============================] - 2s 10ms/step - loss: 0.0215 - val_loss: 0.0245\n",
            "Epoch 10/100\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.0214 - val_loss: 0.0254\n",
            "Epoch 11/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0213 - val_loss: 0.0250\n",
            "Epoch 12/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.0246\n",
            "Epoch 13/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0211 - val_loss: 0.0245\n",
            "Epoch 14/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0211 - val_loss: 0.0251\n",
            "Epoch 15/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0210 - val_loss: 0.0245\n",
            "Epoch 16/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0210 - val_loss: 0.0250\n",
            "Epoch 17/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0209 - val_loss: 0.0256\n",
            "Epoch 18/100\n",
            "183/183 [==============================] - 2s 11ms/step - loss: 0.0209 - val_loss: 0.0255\n",
            "Epoch 19/100\n",
            "183/183 [==============================] - 2s 10ms/step - loss: 0.0209 - val_loss: 0.0245\n",
            "Epoch 20/100\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.0208 - val_loss: 0.0245\n",
            "Epoch 21/100\n",
            "183/183 [==============================] - 1s 8ms/step - loss: 0.0207 - val_loss: 0.0244\n",
            "Epoch 22/100\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.0207 - val_loss: 0.0247\n",
            "Epoch 23/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.0246\n",
            "Epoch 24/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.0242\n",
            "Epoch 25/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0207 - val_loss: 0.0246\n",
            "Epoch 26/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0207 - val_loss: 0.0247\n",
            "Epoch 27/100\n",
            "183/183 [==============================] - 2s 11ms/step - loss: 0.0206 - val_loss: 0.0249\n",
            "Epoch 28/100\n",
            "183/183 [==============================] - 2s 9ms/step - loss: 0.0206 - val_loss: 0.0245\n",
            "Epoch 29/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.0247\n",
            "Epoch 30/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.0241\n",
            "Epoch 31/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0204 - val_loss: 0.0244\n",
            "Epoch 32/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.0245\n",
            "Epoch 33/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0203 - val_loss: 0.0244\n",
            "Epoch 34/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0204 - val_loss: 0.0243\n",
            "Epoch 35/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.0241\n",
            "Epoch 36/100\n",
            "183/183 [==============================] - 2s 8ms/step - loss: 0.0204 - val_loss: 0.0246\n",
            "Epoch 37/100\n",
            "183/183 [==============================] - 2s 9ms/step - loss: 0.0204 - val_loss: 0.0247\n",
            "Epoch 38/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0202 - val_loss: 0.0251\n",
            "Epoch 39/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0203 - val_loss: 0.0248\n",
            "Epoch 40/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0202 - val_loss: 0.0249\n",
            "Epoch 41/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0202 - val_loss: 0.0245\n",
            "Epoch 42/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0249\n",
            "Epoch 43/100\n",
            "183/183 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0248\n",
            "Epoch 44/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0201 - val_loss: 0.0245\n",
            "Epoch 45/100\n",
            "183/183 [==============================] - 1s 7ms/step - loss: 0.0201 - val_loss: 0.0246\n",
            "1/1 [==============================] - 0s 368ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cf7ee875110f>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predicting 31 days with 15-minute intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mjanuary_forecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60,) + inhomogeneous part."
          ]
        }
      ]
    }
  ]
}