{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71ac778448954e9da5b1dd8af0b7670e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc2d2784e6f4462b90a29c40c46eedf9",
              "IPY_MODEL_3623920d1da44a40a5ee8ddd2ac95a22",
              "IPY_MODEL_82364729062345b08013748b095bb279"
            ],
            "layout": "IPY_MODEL_728492e96efb47a98ae94c88aa35cef1"
          }
        },
        "bc2d2784e6f4462b90a29c40c46eedf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e2bfb0f16f94814ba898b8f37b6a51d",
            "placeholder": "​",
            "style": "IPY_MODEL_27484e6e16a54438934773b2c67756bb",
            "value": "Map:   0%"
          }
        },
        "3623920d1da44a40a5ee8ddd2ac95a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db6f77b19a0e465699e638ee28f8e27d",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8769b9b6b3b947a089225c62e3f7e7fc",
            "value": 0
          }
        },
        "82364729062345b08013748b095bb279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae77e0cfc76a4b14adc60d4669580fe9",
            "placeholder": "​",
            "style": "IPY_MODEL_a530b98f59914d63ba0e1387a1898489",
            "value": " 0/100 [00:00&lt;?, ? examples/s]"
          }
        },
        "728492e96efb47a98ae94c88aa35cef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e2bfb0f16f94814ba898b8f37b6a51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27484e6e16a54438934773b2c67756bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db6f77b19a0e465699e638ee28f8e27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8769b9b6b3b947a089225c62e3f7e7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae77e0cfc76a4b14adc60d4669580fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a530b98f59914d63ba0e1387a1898489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58e1211da2a242249473419cbae795fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a25b0c5156304beaacd1c6aed79584c5",
              "IPY_MODEL_c2ddf716acdf49b8b4c4e5e077353b9c",
              "IPY_MODEL_d237091bbfd1491785de8406766e42b1"
            ],
            "layout": "IPY_MODEL_8c36d3f7e65d4c9483c272a52b9291df"
          }
        },
        "a25b0c5156304beaacd1c6aed79584c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c8a34bfd2444629a76d544e8e499c8",
            "placeholder": "​",
            "style": "IPY_MODEL_5b2b4bd84d134b1482adc79496a67c7c",
            "value": "Map:   0%"
          }
        },
        "c2ddf716acdf49b8b4c4e5e077353b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecedc96e5b8c4bc5bb690e3363dd750a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0338835ee8794877bac77c62ffbf66ac",
            "value": 0
          }
        },
        "d237091bbfd1491785de8406766e42b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec64f0c4d5e4e0989e0f66ad44c5147",
            "placeholder": "​",
            "style": "IPY_MODEL_d7d16493e16d41a6b1011ffe431080db",
            "value": " 0/100 [00:00&lt;?, ? examples/s]"
          }
        },
        "8c36d3f7e65d4c9483c272a52b9291df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c8a34bfd2444629a76d544e8e499c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2b4bd84d134b1482adc79496a67c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecedc96e5b8c4bc5bb690e3363dd750a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0338835ee8794877bac77c62ffbf66ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ec64f0c4d5e4e0989e0f66ad44c5147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7d16493e16d41a6b1011ffe431080db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f268e40ea534c43843b0fe3bb9b5744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05095e1df8ba4c4489d375455547ec13",
              "IPY_MODEL_ca439f97a0e5436c96980917cce5d991",
              "IPY_MODEL_4f72f03953b84cd899d7367898f1683e"
            ],
            "layout": "IPY_MODEL_5027f883d9fe490b96bf7364918f8720"
          }
        },
        "05095e1df8ba4c4489d375455547ec13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d2f85781a2649d4a4732344b98586c2",
            "placeholder": "​",
            "style": "IPY_MODEL_cda3abc382df49f8b1914956084b1433",
            "value": "Map: 100%"
          }
        },
        "ca439f97a0e5436c96980917cce5d991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc1897b3d0542e99968bf57c3fdf7c5",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a353acd85414475d8e0c0292f88ab6a5",
            "value": 100
          }
        },
        "4f72f03953b84cd899d7367898f1683e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9190b4bf134678864e63cfef29e666",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7ddad8295d4a19a536556e6abe971a",
            "value": " 100/100 [00:00&lt;00:00, 368.37 examples/s]"
          }
        },
        "5027f883d9fe490b96bf7364918f8720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2f85781a2649d4a4732344b98586c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda3abc382df49f8b1914956084b1433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bc1897b3d0542e99968bf57c3fdf7c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a353acd85414475d8e0c0292f88ab6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c9190b4bf134678864e63cfef29e666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7ddad8295d4a19a536556e6abe971a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5496f480625408a9a971034567590bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4324df90bce845e59d8061371bd0b23f",
              "IPY_MODEL_b4722b0023d740b085402bcfda7a6e37",
              "IPY_MODEL_f743718474bf4d4a8167749419673a0b"
            ],
            "layout": "IPY_MODEL_0c4799d78d804d028619ca2a0047be08"
          }
        },
        "4324df90bce845e59d8061371bd0b23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c66a2a39d8bb4beba160369d865d1cb8",
            "placeholder": "​",
            "style": "IPY_MODEL_4aea68295a0a4ab691a0e8dc6e4add37",
            "value": "model.safetensors: 100%"
          }
        },
        "b4722b0023d740b085402bcfda7a6e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cbb8a3c382a4e9587b4849ef8d96cd3",
            "max": 557709915,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd43c4e7a30c4fe1af0f0a959f61a6a7",
            "value": 557709915
          }
        },
        "f743718474bf4d4a8167749419673a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad72949e50ad46dda2b9248b1ee18b6d",
            "placeholder": "​",
            "style": "IPY_MODEL_3736f14255b44e6ab886c191f3e5542e",
            "value": " 558M/558M [00:03&lt;00:00, 164MB/s]"
          }
        },
        "0c4799d78d804d028619ca2a0047be08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66a2a39d8bb4beba160369d865d1cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aea68295a0a4ab691a0e8dc6e4add37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cbb8a3c382a4e9587b4849ef8d96cd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd43c4e7a30c4fe1af0f0a959f61a6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad72949e50ad46dda2b9248b1ee18b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3736f14255b44e6ab886c191f3e5542e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "674b179f334547c6a07985466b7e0f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0baf98ff48e49a78abdc209e25b7232",
              "IPY_MODEL_a6b4bd3e91e544c7a8e029654ff1bd2b",
              "IPY_MODEL_40eba478c3344456906f903a70cf7d57"
            ],
            "layout": "IPY_MODEL_faaefa7572014e279a5a5827d2d35788"
          }
        },
        "b0baf98ff48e49a78abdc209e25b7232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a1e6de288964985ad3fbf631fa3500c",
            "placeholder": "​",
            "style": "IPY_MODEL_bba79497b99345e0b2628a42811410b9",
            "value": "  0%"
          }
        },
        "a6b4bd3e91e544c7a8e029654ff1bd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf9f572be07c4567be336b0978c285f1",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5e98504ad0d487c9751ed1a95d8aa58",
            "value": 0
          }
        },
        "40eba478c3344456906f903a70cf7d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55475dac93c842c0a3e0072483922b2e",
            "placeholder": "​",
            "style": "IPY_MODEL_cac8a68c63bc472fb7ea657d1abbd8de",
            "value": " 0/25 [00:00&lt;?, ?it/s]"
          }
        },
        "faaefa7572014e279a5a5827d2d35788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a1e6de288964985ad3fbf631fa3500c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba79497b99345e0b2628a42811410b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf9f572be07c4567be336b0978c285f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e98504ad0d487c9751ed1a95d8aa58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55475dac93c842c0a3e0072483922b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cac8a68c63bc472fb7ea657d1abbd8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G1Oz9DIWHt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14eaf123-0c47-4c02-a5fc-8e6666de2660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-21 03:29:06.144 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-06-21 03:29:06.162 Session state does not function when running a script without `streamlit run`\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"\"))\n",
        "\n",
        "## function to load Gemini Pro model and get repsonses\n",
        "model=genai.GenerativeModel(\"gemini-pro\")\n",
        "chat = model.start_chat(history=[])\n",
        "def get_gemini_response(question):\n",
        "\n",
        "    response=chat.send_message(question,stream=True)\n",
        "    return response\n",
        "\n",
        "##initialize our streamlit app\n",
        "\n",
        "st.set_page_config(page_title=\"Q&A Demo\")\n",
        "\n",
        "st.header(\"Gemini LLM Application\")\n",
        "\n",
        "# Initialize session state for chat history if it doesn't exist\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state['chat_history'] = []\n",
        "\n",
        "input=st.text_input(\"Input: \",key=\"input\")\n",
        "submit=st.button(\"Ask the question\")\n",
        "\n",
        "if submit and input:\n",
        "    response=get_gemini_response(input)\n",
        "    # Add user query and response to session state chat history\n",
        "    st.session_state['chat_history'].append((\"You\", input))\n",
        "    st.subheader(\"The Response is\")\n",
        "    for chunk in response:\n",
        "        st.write(chunk.text)\n",
        "        st.session_state['chat_history'].append((\"Bot\", chunk.text))\n",
        "st.subheader(\"The Chat History is\")\n",
        "\n",
        "for role, text in st.session_state['chat_history']:\n",
        "    st.write(f\"{role}: {text}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit google-generativeai pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga-i3gLq2aJY",
        "outputId": "26562bee-315a-4db9-bb95-71ff136d675b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"\"))\n",
        "\n",
        "## function to load Gemini Pro model and get repsonses\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "chat = model.start_chat(history=[])\n",
        "def get_gemini_response(question):\n",
        "    response = chat.send_message(question, stream=True)\n",
        "    return response\n",
        "\n",
        "##initialize our streamlit app\n",
        "st.set_page_config(page_title=\"Q&A Demo\")\n",
        "\n",
        "st.header(\"Gemini LLM Application\")\n",
        "\n",
        "# Initialize session state for chat history if it doesn't exist\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state['chat_history'] = []\n",
        "\n",
        "input = st.text_input(\"Input: \", key=\"input\")\n",
        "submit = st.button(\"Ask the question\")\n",
        "\n",
        "if submit and input:\n",
        "    response = get_gemini_response(input)\n",
        "    # Add user query and response to session state chat history\n",
        "    st.session_state['chat_history'].append((\"You\", input))\n",
        "    st.subheader(\"The Response is\")\n",
        "    for chunk in response:\n",
        "        st.write(chunk.text)\n",
        "        st.session_state['chat_history'].append((\"Bot\", chunk.text))\n",
        "\n",
        "st.subheader(\"The Chat History is\")\n",
        "for role, text in st.session_state['chat_history']:\n",
        "    st.write(f\"{role}: {text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sD8yTY2fQa",
        "outputId": "8fd820f5-a685-4dc3-e940-e6edfea37f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"\"))\n",
        "\n",
        "## function to load Gemini Pro model and get responses\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "def get_gemini_response(question):\n",
        "    response = chat.send_message(question, stream=True)\n",
        "    full_response = \"\".join([chunk.text for chunk in response])\n",
        "    return full_response\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_interface(input_text):\n",
        "    response = get_gemini_response(input_text)\n",
        "    return response\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(fn=gradio_interface,\n",
        "                         inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "                         outputs=\"text\",\n",
        "                         title=\"Gemini LLM Application\",\n",
        "                         description=\"Ask questions and get responses from the Gemini LLM.\")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YIcN9Qvy2iKA",
        "outputId": "c8fa054b-e9bc-43eb-ec7a-a28170911e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gradio' has no attribute 'inputs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1fb1d494491d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create a Gradio interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m interface = gr.Interface(fn=gradio_interface,\n\u001b[0;32m---> 26\u001b[0;31m                          \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Enter your question here...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                          \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                          \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gemini LLM Application\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'inputs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai translate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc5VWnnJ3fOh",
        "outputId": "23a35073-322d-40b4-e13d-0f91863cb987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.31.0)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.6.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(data.head())\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "q5ow5CkO3gho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04a0c7f-1d07-43ca-eea6-6ccc200925ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Timestamp  Main incomer Current R  Main incomer Current Y  \\\n",
            "0  01-08-2023 00:00              173.042465              173.933060   \n",
            "1  01-08-2023 00:15              136.713852              136.000031   \n",
            "2  01-08-2023 00:30              122.184158              120.531586   \n",
            "3  01-08-2023 00:45              147.820511              147.056381   \n",
            "4  01-08-2023 01:00              169.530563              169.660965   \n",
            "\n",
            "   Main incomer Current B  Main incomer VOL RY  Main incomer VOL YB  \\\n",
            "0              182.334091           420.092926           421.757599   \n",
            "1              142.112228           422.147919           423.574371   \n",
            "2              129.242294           423.314911           424.781525   \n",
            "3              154.585083           423.466125           425.228546   \n",
            "4              180.001251           424.328369           426.126343   \n",
            "\n",
            "   Main incomer VOL BR  Main incomer KW  Main incomer PF  \n",
            "0           422.483429              NaN        85.850424  \n",
            "1           424.100830              NaN        82.693070  \n",
            "2           425.750183              NaN        78.835297  \n",
            "3           425.842743              NaN        81.879675  \n",
            "4           426.804993              NaN        85.423458  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15308 entries, 0 to 15307\n",
            "Data columns (total 9 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   Timestamp               11674 non-null  object \n",
            " 1   Main incomer Current R  11674 non-null  float64\n",
            " 2   Main incomer Current Y  11674 non-null  float64\n",
            " 3   Main incomer Current B  11674 non-null  float64\n",
            " 4   Main incomer VOL RY     11674 non-null  float64\n",
            " 5   Main incomer VOL YB     11674 non-null  float64\n",
            " 6   Main incomer VOL BR     11674 non-null  float64\n",
            " 7   Main incomer KW         0 non-null      float64\n",
            " 8   Main incomer PF         11674 non-null  float64\n",
            "dtypes: float64(8), object(1)\n",
            "memory usage: 1.1+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "file_path = '/path_to_your_file.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Inspect the data\n",
        "print(data.head())\n",
        "print(data.columns)\n",
        "\n",
        "# Assuming the dataset does not have a 'Date' column, we'll proceed with indexing by rows\n",
        "# Handle missing values\n",
        "data = data.fillna(method='ffill')\n",
        "\n",
        "# Assuming the column to be forecasted is 'Value' (adjust if the name is different)\n",
        "values = data['Value'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(values)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequences.append(data[i:i + sequence_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "sequence_length = 5  # Example sequence length\n",
        "sequences = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "# Split the data\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data = sequences[:train_size]\n",
        "test_data = sequences[train_size:]\n",
        "\n",
        "# Separate features and labels\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1, 0]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1, 0]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length - 1, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Generate predictions for each day in January\n",
        "january_predictions = []\n",
        "input_sequence = scaled_data[-sequence_length:]\n",
        "\n",
        "for _ in range(31):  # Assuming we want to predict for 31 days in January\n",
        "    input_sequence_reshaped = input_sequence.reshape((1, sequence_length, 1))\n",
        "    next_prediction = model.predict(input_sequence_reshaped)\n",
        "    january_predictions.append(next_prediction[0, 0])\n",
        "\n",
        "    # Update the input sequence to include the latest prediction\n",
        "    input_sequence = np.append(input_sequence[1:], next_prediction)\n",
        "\n",
        "# Inverse transform the predictions to get them back to original scale\n",
        "january_predictions = scaler.inverse_transform(np.array(january_predictions).reshape(-1, 1))\n",
        "\n",
        "# Plot the predictions\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(range(len(values)), values, label='Actual Data')\n",
        "plt.plot(range(len(values), len(values) + 31), january_predictions, label='January Predictions', color='red')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('January Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "493j_qbHeU_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Timestamp' column to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
        "\n",
        "# Filter the data for August to December\n",
        "filtered_data = data[(data['Timestamp'].dt.month >= 8) & (data['Timestamp'].dt.month <= 12)]\n",
        "\n",
        "# Handle missing values\n",
        "filtered_data = filtered_data.fillna(method='ffill')\n",
        "\n",
        "# Use the 'Main incomer PF' column for predictions\n",
        "values = filtered_data['Main incomer Current R '].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(values)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequences.append(data[i:i + sequence_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "sequence_length = 5  # Example sequence length\n",
        "sequences = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "# Split the data\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data = sequences[:train_size]\n",
        "test_data = sequences[train_size:]\n",
        "\n",
        "# Separate features and labels\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1, 0]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1, 0]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length - 1, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Generate predictions for each day in January\n",
        "january_predictions = []\n",
        "input_sequence = scaled_data[-sequence_length:]\n",
        "\n",
        "for _ in range(31):  # Assuming we want to predict for 31 days in January\n",
        "    input_sequence_reshaped = input_sequence.reshape((1, sequence_length, 1))\n",
        "    next_prediction = model.predict(input_sequence_reshaped)\n",
        "    january_predictions.append(next_prediction[0, 0])\n",
        "\n",
        "    # Update the input sequence to include the latest prediction\n",
        "    input_sequence = np.append(input_sequence[1:], next_prediction)\n",
        "\n",
        "# Inverse transform the predictions to get them back to original scale\n",
        "january_predictions = scaler.inverse_transform(np.array(january_predictions).reshape(-1, 1))\n",
        "\n",
        "# Plot the predictions\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(filtered_data['Timestamp'], values, label='Actual Data')\n",
        "plt.plot(pd.date_range(start='2024-01-01', periods=31, freq='D'), january_predictions, label='January Predictions', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Main incomer PF')\n",
        "plt.title('January Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "ZrFJbQnSehdn",
        "outputId": "8e0836de-5dff-4c3d-8941-0cbc8e5b7066"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "time data \"13-08-2023 00:00\" doesn't match format \"%m-%d-%Y %H:%M\", at position 1140. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c06c22f36808>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Convert the 'Timestamp' column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Filter the data for August to December\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     result, tz_parsed = objects_to_datetime64ns(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mCall\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfallback\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m'errors'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimezones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_return_parsed_timezone_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/tslibs/strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/tslibs/strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: time data \"13-08-2023 00:00\" doesn't match format \"%m-%d-%Y %H:%M\", at position 1140. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Timestamp' column to datetime with the correct format\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Filter the data for August to December\n",
        "filtered_data = data[(data['Timestamp'].dt.month >= 8) & (data['Timestamp'].dt.month <= 12)]\n",
        "\n",
        "# Handle missing values\n",
        "filtered_data = filtered_data.fillna(method='ffill')\n",
        "\n",
        "# Use the 'Main incomer PF' column for predictions\n",
        "values = filtered_data['Main incomer Current R '].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(values)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequences.append(data[i:i + sequence_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "sequence_length = 5  # Example sequence length\n",
        "sequences = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "# Split the data\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data = sequences[:train_size]\n",
        "test_data = sequences[train_size:]\n",
        "\n",
        "# Separate features and labels\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1, 0]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1, 0]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length - 1, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Generate predictions for each day in January\n",
        "january_predictions = []\n",
        "input_sequence = scaled_data[-sequence_length + 1:]\n",
        "\n",
        "for _ in range(31):  # Assuming we want to predict for 31 days in January\n",
        "    input_sequence_reshaped = input_sequence.reshape((1, sequence_length -1, 1))\n",
        "    next_prediction = model.predict(input_sequence_reshaped)\n",
        "    january_predictions.append(next_prediction[0, 0])\n",
        "\n",
        "    # Update the input sequence to include the latest prediction\n",
        "    input_sequence = np.append(input_sequence[1:], next_prediction)\n",
        "\n",
        "# Inverse transform the predictions to get them back to original scale\n",
        "january_predictions = scaler.inverse_transform(np.array(january_predictions).reshape(-1, 1))\n",
        "\n",
        "# Plot the predictions\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(filtered_data['Timestamp'], values, label='Actual Data')\n",
        "plt.plot(pd.date_range(start='2024-01-01', periods=31, freq='D'), january_predictions, label='January Predictions', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Main incomer PF')\n",
        "plt.title('January Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "w-1gXaYyfXjX",
        "outputId": "69f3dfab-34f2-40cb-f6e8-8cabbe67c911"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Main incomer Current R '",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Main incomer Current R '",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-26139d14877c>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Use the 'Main incomer PF' column for predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Main incomer Current R '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Scale the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Main incomer Current R '"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Timestamp' column to datetime with the correct format\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Filter the data for August to December\n",
        "filtered_data = data[(data['Timestamp'].dt.month >= 8) & (data['Timestamp'].dt.month <= 12)]\n",
        "\n",
        "# Handle missing values\n",
        "filtered_data = filtered_data.fillna(method='ffill')\n",
        "\n",
        "# Use the 'Main incomer Current R' column for predictions (remove trailing space)\n",
        "values = filtered_data['Main incomer Current R'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(values)\n",
        "\n",
        "# Create sequences function adjusted for LSTM\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length - 1])  # Features (sequence_length - 1)\n",
        "        y.append(data[i + sequence_length - 1])    # Label (next value after sequence)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 5  # Example sequence length\n",
        "X, y = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "# Split the data\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length - 1, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Generate predictions for each day in January\n",
        "january_predictions_scaled = []\n",
        "input_sequence = scaled_data[-sequence_length:]\n",
        "\n",
        "for _ in range(31):  # Assuming we want to predict for 31 days in January\n",
        "    input_sequence_reshaped = input_sequence.reshape((1, sequence_length - 1, 1))\n",
        "    next_prediction = model.predict(input_sequence_reshaped)\n",
        "    january_predictions_scaled.append(next_prediction[0, 0])\n",
        "\n",
        "    # Update the input sequence to include the latest prediction\n",
        "    input_sequence = np.append(input_sequence[1:], next_prediction)\n",
        "\n",
        "# Inverse transform the predictions to get them back to original scale\n",
        "january_predictions = scaler.inverse_transform(np.array(january_predictions_scaled).reshape(-1, 1))\n",
        "\n",
        "# Plot the predictions\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(filtered_data['Timestamp'], values, label='Actual Data')\n",
        "plt.plot(pd.date_range(start='2024-08-01', periods=len(values), freq='D'), values, label='Actual Data', color='blue')\n",
        "plt.plot(pd.date_range(start='2024-08-01', periods=len(january_predictions), freq='D'), january_predictions, label='January Predictions', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Main incomer Current R')\n",
        "plt.title('January Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "niNaxedFjCCp",
        "outputId": "7a4f72e0-8a23-485e-98b2-9dbe104c0b05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "292/292 [==============================] - 7s 8ms/step - loss: 0.0174\n",
            "Epoch 2/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0085\n",
            "Epoch 3/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0079\n",
            "Epoch 4/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0076\n",
            "Epoch 5/20\n",
            "292/292 [==============================] - 3s 9ms/step - loss: 0.0074\n",
            "Epoch 6/20\n",
            "292/292 [==============================] - 3s 10ms/step - loss: 0.0072\n",
            "Epoch 7/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0072\n",
            "Epoch 8/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0071\n",
            "Epoch 9/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0072\n",
            "Epoch 10/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0072\n",
            "Epoch 11/20\n",
            "292/292 [==============================] - 3s 11ms/step - loss: 0.0071\n",
            "Epoch 12/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0072\n",
            "Epoch 13/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0071\n",
            "Epoch 14/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0071\n",
            "Epoch 15/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0071\n",
            "Epoch 16/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0071\n",
            "Epoch 17/20\n",
            "292/292 [==============================] - 3s 11ms/step - loss: 0.0071\n",
            "Epoch 18/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0071\n",
            "Epoch 19/20\n",
            "292/292 [==============================] - 2s 8ms/step - loss: 0.0071\n",
            "Epoch 20/20\n",
            "292/292 [==============================] - 2s 7ms/step - loss: 0.0071\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 5 into shape (1,4,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-28c636f14023>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Assuming we want to predict for 31 days in January\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0minput_sequence_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mnext_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mjanuary_predictions_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 5 into shape (1,4,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/Main_incomer_MFM_2024-02-07-18-59-33.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Timestamp' column to datetime with the correct format\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "# Filter the data for August to December\n",
        "filtered_data = df[(df['Timestamp'].dt.month >= 8) & (df['Timestamp'].dt.month <= 12)]\n",
        "\n",
        "# Handle missing values if any\n",
        "filtered_data = filtered_data.fillna(method='ffill')\n",
        "\n",
        "# Use the 'Main incomer Current R' column for predictions (remove trailing space)\n",
        "data = filtered_data['Main incomer Current R'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "# Function to create sequences of data\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequence = data[i:i+sequence_length]\n",
        "        label = data[i+sequence_length]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "sequence_length = 60  # e.g., using past 60 time steps to predict the next step\n",
        "X_train, y_train = create_sequences(train_data, sequence_length)\n",
        "X_test, y_test = create_sequences(test_data, sequence_length)\n",
        "\n",
        "# Reshape the data for LSTM [samples, time steps, features]\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions to get actual values\n",
        "predicted_values = scaler.inverse_transform(predicted_values)\n",
        "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = np.mean(np.square(y_test_actual - predicted_values))\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(filtered_data.index[:train_size], scaler.inverse_transform(train_data), label='Training Data', color='blue')\n",
        "\n",
        "plt.plot(filtered_data.index[train_size+sequence_length:], predicted_values, label='Predicted Test Data', color='red')\n",
        "plt.title('Main Incomer Current R Forecast using LSTM')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Main Incomer Current R')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iCeDUha73v3E",
        "outputId": "dd8d5f54-e025-48ec-d5c2-debae76aa085"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "232/232 [==============================] - 44s 146ms/step - loss: 0.0135 - val_loss: 0.0093\n",
            "Epoch 2/20\n",
            "232/232 [==============================] - 27s 117ms/step - loss: 0.0085 - val_loss: 0.0080\n",
            "Epoch 3/20\n",
            "232/232 [==============================] - 15s 65ms/step - loss: 0.0078 - val_loss: 0.0079\n",
            "Epoch 4/20\n",
            "232/232 [==============================] - 15s 64ms/step - loss: 0.0074 - val_loss: 0.0071\n",
            "Epoch 5/20\n",
            "232/232 [==============================] - 15s 67ms/step - loss: 0.0074 - val_loss: 0.0071\n",
            "Epoch 6/20\n",
            "232/232 [==============================] - 17s 73ms/step - loss: 0.0072 - val_loss: 0.0072\n",
            "Epoch 7/20\n",
            "232/232 [==============================] - 15s 66ms/step - loss: 0.0071 - val_loss: 0.0069\n",
            "Epoch 8/20\n",
            "232/232 [==============================] - 17s 73ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 9/20\n",
            "232/232 [==============================] - 16s 68ms/step - loss: 0.0071 - val_loss: 0.0067\n",
            "Epoch 10/20\n",
            "232/232 [==============================] - 15s 66ms/step - loss: 0.0071 - val_loss: 0.0069\n",
            "Epoch 11/20\n",
            "232/232 [==============================] - 16s 67ms/step - loss: 0.0070 - val_loss: 0.0069\n",
            "Epoch 12/20\n",
            "232/232 [==============================] - 17s 74ms/step - loss: 0.0071 - val_loss: 0.0066\n",
            "Epoch 13/20\n",
            "232/232 [==============================] - 16s 70ms/step - loss: 0.0070 - val_loss: 0.0066\n",
            "Epoch 14/20\n",
            "232/232 [==============================] - 16s 67ms/step - loss: 0.0070 - val_loss: 0.0068\n",
            "Epoch 15/20\n",
            "232/232 [==============================] - 15s 66ms/step - loss: 0.0070 - val_loss: 0.0066\n",
            "Epoch 16/20\n",
            "232/232 [==============================] - 16s 67ms/step - loss: 0.0070 - val_loss: 0.0066\n",
            "Epoch 17/20\n",
            "232/232 [==============================] - 16s 68ms/step - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 18/20\n",
            "232/232 [==============================] - 16s 69ms/step - loss: 0.0069 - val_loss: 0.0065\n",
            "Epoch 19/20\n",
            "232/232 [==============================] - 16s 68ms/step - loss: 0.0069 - val_loss: 0.0068\n",
            "Epoch 20/20\n",
            "232/232 [==============================] - 16s 69ms/step - loss: 0.0069 - val_loss: 0.0067\n",
            "72/72 [==============================] - 2s 18ms/step\n",
            "Mean Squared Error: 658.01541786608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7dcf88dca950>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df = pd.DataFrame({\n",
        "    'Date': filtered_data['Timestamp'].values[train_size + sequence_length:],\n",
        "    'Predicted_Main_Incomer_Current_R': predicted_values.reshape(-1)\n",
        "})\n",
        "\n",
        "# Save DataFrame to Excel\n",
        "excel_file = '/content/Predicted_Main_Incomer_Current_R.xlsx'\n",
        "predicted_df.to_excel(excel_file, index=False)\n",
        "print(f'Predictions saved to {excel_file}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cOqzEOkChN0",
        "outputId": "a53615dc-078e-44fd-a5e7-1ce3106c69ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/Predicted_Main_Incomer_Current_R.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Date' column to datetime\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Filter the data for August to December\n",
        "data = data[(data['Date'].dt.month >= 8) & (data['Date'].dt.month <= 12)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "HYI-ThtYdf6W",
        "outputId": "6279b670-942a-4a4c-8865-63ead43e2232"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c59802097835>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert the 'Date' column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Filter the data for August to December\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "from translate import Translator\n",
        "\n",
        "# Set the Gemini API key (replace with your actual key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "def extract_locations(user_input_text):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input_text)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the Gemini API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = genai.generate_text(\n",
        "        prompt,\n",
        "        model=\"gemini-pro\",\n",
        "        max_output_tokens=500\n",
        "    )\n",
        "\n",
        "    return response.text.strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each sentence and join them back together\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = translator.translate(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    # Use a different variable name to avoid overwriting the input() function\n",
        "    user_input_text = input(\"Enter your request: \")\n",
        "    if user_input_text.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input_text)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "    # Ask user for desired language for route description\n",
        "    target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "\n",
        "    translated_description = translate_description(route_description, target_language)\n",
        "    print(f\"Route Description (in {target_language}):\", translated_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "V7e8AXK23iuY",
        "outputId": "f03d4b50-51f4-4d4f-e38f-d159aa9c95fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-19dad359e0fa>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Use a different variable name to avoid overwriting the input() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0muser_input_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from translate import Translator\n",
        "\n",
        "# Set the Gemini API key (replace with your actual key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
      ],
      "metadata": {
        "id": "dF7xKkBn5VnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the Gemini API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = genai.generate_text(\n",
        "        prompt,\n",
        "        model=\"gemini-pro\",\n",
        "        max_output_tokens=500\n",
        "    )\n",
        "\n",
        "    return response.text.strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = [translator.translate(sentence) for sentence in sentences]\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n"
      ],
      "metadata": {
        "id": "g6gBgChG5Z-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_locations(user_input_text):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input_text)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input_text = input(\"Enter your request: \")\n",
        "    if user_input_text.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input_text)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "    # Ask user for desired language for route description\n",
        "    target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "\n",
        "    translated_description = translate_description(route_description, target_language)\n",
        "    print(f\"Route Description (in {target_language}):\", translated_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "RGuZ1yLe5dQA",
        "outputId": "98d68ad7-484b-43d4-e229-7feb96dcee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f3da1530da83>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0muser_input_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-GYPMe15z_I",
        "outputId": "3d18707d-93d6-44d2-c361-c3c13a1d76f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m143.4/163.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/717.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.3/717.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "GOOGLE_API_KEY = \"\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "6DzfxLIL54GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai google-generativeai translate pydub SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDntr4iH-k1s",
        "outputId": "090259b5-fceb-4732-e82e-f85810e4ee60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/327.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: translate in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.5 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.5)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.5->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.31.0)\n",
            "Requirement already satisfied: libretranslatepy==2.1.1 in /usr/local/lib/python3.10/dist-packages (from translate) (2.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.0.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Installing collected packages: SpeechRecognition, openai\n",
            "Successfully installed SpeechRecognition-3.10.4 openai-1.35.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each sentence and join them back together\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = translator.translate(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio.export(\"temp.wav\", format=\"wav\")\n",
        "    with sr.AudioFile(\"temp.wav\") as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        text = recognizer.recognize_whisper(audio_data, model=\"base\")\n",
        "    return text\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    # Assuming audio file input for simplicity. You can change this to live audio recording if needed.\n",
        "    audio_path = input(\"Enter the path to the audio file (or type 'exit' to quit): \")\n",
        "    if audio_path.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    user_input = transcribe_audio(audio_path)\n",
        "    print(f\"Transcribed Text: {user_input}\")\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "    # Ask user for desired language for route description\n",
        "    target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "\n",
        "    translated_description = translate_description(route_description, target_language)\n",
        "    print(f\"Route Description (in {target_language}):\", translated_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIVfzlrq-uQ5",
        "outputId": "3b5d5c44-3b83-4ebe-c477-2dafd0bd88ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\n",
            "Enter the path to the audio file (or type 'exit' to quit): /content/record1.in-speech.mp3\n",
            "Transcribed Text:  Provide route from Mumbai to Puna.\n",
            "Route Description (in English): Sure! The driving route from Mumbai to Pune is approximately 150 kilometers and can take around 3 to 4 hours, depending on traffic conditions. Here's a detailed route with major turns, landmarks, and distances:\n",
            "\n",
            "1. Start in Mumbai and head towards the Eastern Express Highway.\n",
            "\n",
            "2. Follow the Eastern Express Highway for about 25 kilometers until you reach the town of Thane.\n",
            "\n",
            "3. After crossing Thane, continue on the Eastern Express Highway towards the Majiwada Junction.\n",
            "\n",
            "4. At Majiwada Junction, take a left turn onto the Mumbai-Nashik Expressway and continue for about 10 kilometers.\n",
            "\n",
            "5. Keep following the Mumbai-Nashik Expressway until you reach the Khalapur Toll Plaza at around 65 kilometers from Mumbai.\n",
            "\n",
            "6. After crossing the toll plaza, continue on the Mumbai-Pune Expressway for another 30 kilometers until you reach Lonavala.\n",
            "\n",
            "7. In Lonavala, take the exit towards Pune and continue on the old Pune-Mumbai Highway.\n",
            "\n",
            "8. Follow the Pune-Mumbai Highway for about 25 kilometers until you reach Pune city.\n",
            "\n",
            "9. Once in Pune, follow the signs towards your specific destination within the city.\n",
            "\n",
            "Key Landmarks:\n",
            "- Thane\n",
            "- Majiwada Junction\n",
            "- Khalapur Toll Plaza\n",
            "- Lonavala\n",
            "\n",
            "Please note that the route may be subject to changes due to road conditions or construction. It's always a good idea to have a map or GPS navigation system handy during your journey. Drive safely!\n",
            "Enter language code for translation (e.g., 'hi' for Hindi): hi\n",
            "Route Description (in hi): ज़रूर! मुंबई से पुणे तक का ड्राइविंग रूट लगभग 150 किलोमीटर है और ट्रैफ़िक की स्थिति के आधार पर इसमें लगभग 3 से 4 घंटे लग सकते हैं. यहाँ प्रमुख मोड़, लैंडमार्क और दूरी के साथ एक विस्तृत मार्ग दिया गया है:\n",
            "\n",
            "1. मुंबई में शुरू करें और ईस्टर्न एक्सप्रेस हाईवे की ओर बढ़ें।\n",
            "\n",
            "2. ठाणे शहर तक पहुँचने तक लगभग 25 किलोमीटर तक ईस्टर्न एक्सप्रेस हाईवे का पालन करें।\n",
            "\n",
            "3. ठाणे को पार करने के बाद, मजीवाड़ा जंक्शन की ओर पूर्वी एक्सप्रेस राजमार्ग पर जारी रखें।\n",
            "\n",
            "4. मजीवाड़ा जंक्शन पर, मुंबई - नासिक एक्सप्रेसवे पर बाएं मुड़ें और लगभग 10 किलोमीटर तक जारी रखें।\n",
            "\n",
            "5. मुंबई से लगभग 65 किलोमीटर की दूरी पर खालापुर टोल प्लाजा तक पहुंचने तक मुंबई - नासिक एक्सप्रेसवे का पालन करते रहें।\n",
            "\n",
            "6. टोल प्लाज़ा पार करने के बाद, लोनावाला पहुँचने तक 30 किलोमीटर तक मुंबई - पुणे एक्सप्रेस - वे पर बने रहें।\n",
            "\n",
            "7. लोनावाला में, पुणे की ओर निकलें और पुराने पुणे - मुंबई राजमार्ग पर जारी रखें।\n",
            "\n",
            "8. पुणे शहर पहुंचने तक लगभग 25 किलोमीटर तक पुणे - मुंबई राजमार्ग का पालन करें।\n",
            "\n",
            "9. पुणे में एक बार, शहर के भीतर अपने विशिष्ट गंतव्य की ओर संकेतों का पालन करें।\n",
            "\n",
            "प्रमुख स्थल :-\n",
            "ठाणे -\n",
            "मजीवाड़ा जंक्शन\n",
            "- खालापुर टोल प्लाजा\n",
            "- लोनावाला\n",
            "\n",
            "कृपया ध्यान दें कि सड़क की स्थिति या निर्माण के कारण मार्ग में बदलाव हो सकते हैं. अपनी यात्रा के दौरान मैप या जीपीएस नेविगेशन सिस्टम का इस्तेमाल करना हमेशा अच्छा होता है. सुरक्षित तरीके से गाड़ी चलाएँ!\n",
            "Enter the path to the audio file (or type 'exit' to quit): exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each sentence and join them back together\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = translator.translate(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio.export(\"temp.wav\", format=\"wav\")\n",
        "    with sr.AudioFile(\"temp.wav\") as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        text = recognizer.recognize_whisper(audio_data, model=\"base\")\n",
        "    return text\n",
        "\n",
        "# Run the chatbot\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students.\")\n",
        "while True:\n",
        "    input_type = input(\"Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): \").lower()\n",
        "\n",
        "    if input_type == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    elif input_type == 'text':\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "            continue\n",
        "\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "        target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "        translated_description = translate_description(route_description, target_language)\n",
        "        print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "    elif input_type == 'audio':\n",
        "        audio_path = input(\"Enter the path to the audio file: \")\n",
        "\n",
        "        try:\n",
        "            user_input = transcribe_audio(audio_path)\n",
        "            print(f\"Transcribed Text: {user_input}\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Invalid input format from audio transcription.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "            target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "            translated_description = translate_description(route_description, target_language)\n",
        "            print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'text' or 'audio'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v-ER4ltD0OA",
        "outputId": "fdce44eb-a291-4341-a99b-1940e5ed2c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students.\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): text\n",
            "Enter your request: provide route from kannur to kozhikode\n",
            "Route Description (in English): Certainly! The driving route from Kannur to Kozhikode is about 95 kilometers long and takes approximately 2 to 3 hours, depending on traffic conditions. Here is a detailed route with major turns, landmarks, and approximate distances:\n",
            "\n",
            "1. Start from Kannur and head south on NH 66 (also known as Kochi-Panvel Highway).\n",
            "2. After about 10 kilometers, you will pass through Thalassery. Continue on NH 66.\n",
            "3. About 30 kilometers from Kannur, you will reach Mahe. Stay on NH 66.\n",
            "4. After driving about 30 more kilometers, you will approach Vadakara. Continue on NH 66.\n",
            "5. Around 10 kilometers after Vadakara, you will reach the Feroke Bridge on the outskirts of Kozhikode city.\n",
            "6. Cross the Feroke Bridge and continue straight on NH 66.\n",
            "7. Drive for another 10 kilometers, and you will reach Kozhikode city center.\n",
            "\n",
            "Major landmarks along the way:\n",
            "- Thalassery: Famous for its historic fort and beaches.\n",
            "- Mahe: A small town with a unique blend of French and Indian cultures.\n",
            "- Vadakara: Known for its historical significance and temples.\n",
            "- Feroke Bridge: A prominent bridge that marks the entrance to Kozhikode city.\n",
            "\n",
            "Please note that these distances and landmarks are approximate and may vary based on your exact starting point and traffic conditions. Make sure to follow road signs and drive carefully throughout your journey.\n",
            "Enter language code for translation (e.g., 'hi' for Hindi): hi\n",
            "Route Description (in hi): निश्चित रूप से! कन्नूर से कोझिकोड तक का ड्राइविंग मार्ग लगभग 95 किलोमीटर लंबा है और यातायात की स्थिति के आधार पर इसमें लगभग 2 से 3 घंटे लगते हैं. यहाँ प्रमुख मोड़, लैंडमार्क और अनुमानित दूरी के साथ एक विस्तृत मार्ग दिया गया है:\n",
            "\n",
            "1. कन्नूर से शुरू करें और राष्ट्रीय राजमार्ग 66 (जिसे कोच्चि - पनवेल राजमार्ग भी कहा जाता है) पर दक्षिण की ओर जाएं।\n",
            "2. लगभग 10 किलोमीटर के बाद, आप थलासरी से गुजरेंगे. NH 66. 3 पर जारी रखें. कन्नूर से लगभग 30 किलोमीटर दूर, आप माहे पहुंचेंगे. NH 66.4 पर ठहरें. लगभग 30 किलोमीटर और ड्राइव करने के बाद, आप वडाकारा से संपर्क करेंगे. NH 66.5 पर जारी रखें. वडाकारा से लगभग 10 किलोमीटर बाद, आप कोझिकोड शहर के बाहरी इलाके में फेरोक ब्रिज तक पहुँचेंगे।\n",
            "6. फेरोक ब्रिज को पार करें और सीधे NH 66.7 पर जारी रखें. QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS. पूरी यात्रा के दौरान सड़क के संकेतों का पालन करना और सावधानी से गाड़ी चलाना न भूलें।\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    def chunks(lst, n):\n",
        "        for i in range(0, len(lst), n):\n",
        "            yield lst[i:i + n]\n",
        "\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each chunk of sentences and join them back together\n",
        "    for chunk in chunks(sentences, 5):  # Limiting to 5 sentences per chunk to avoid length issues\n",
        "        chunk_text = '. '.join(chunk)\n",
        "        translated_chunk = translator.translate(chunk_text)\n",
        "        translated_sentences.append(translated_chunk)\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio.export(\"temp.wav\", format=\"wav\")\n",
        "    with sr.AudioFile(\"temp.wav\") as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        text = recognizer.recognize_whisper(audio_data, model=\"base\")\n",
        "    return text\n",
        "\n",
        "# Run the chatbot\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students.\")\n",
        "while True:\n",
        "    input_type = input(\"Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): \").lower()\n",
        "\n",
        "    if input_type == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    elif input_type == 'text':\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "            continue\n",
        "\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "        if len(route_description) > 500:\n",
        "            print(\"Route description exceeds 500 characters. Please provide a shorter request.\")\n",
        "            continue\n",
        "\n",
        "        target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "        translated_description = translate_description(route_description, target_language)\n",
        "        print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "    elif input_type == 'audio':\n",
        "        audio_path = input(\"Enter the path to the audio file: \")\n",
        "\n",
        "        try:\n",
        "            user_input = transcribe_audio(audio_path)\n",
        "            print(f\"Transcribed Text: {user_input}\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Invalid input format from audio transcription.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "            if len(route_description) > 500:\n",
        "                print(\"Route description exceeds 500 characters. Please provide a shorter request.\")\n",
        "                continue\n",
        "\n",
        "            target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "            translated_description = translate_description(route_description, target_language)\n",
        "            print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'text' or 'audio'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc9HNBX0EO8R",
        "outputId": "8a6a4692-a1c6-4f84-aff4-6af941db7692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students.\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): audio\n",
            "Enter the path to the audio file: /content/record1.in-speech.mp3\n",
            "Transcribed Text:  Provide route from Mumbai to Puna.\n",
            "Route Description (in English): Sure! The route from Mumbai to Pune is approximately 150 kilometers and can take around 3 to 4 hours, depending on traffic conditions. Here is a detailed driving route from Mumbai to Pune:\n",
            "\n",
            "1. **Starting Point - Mumbai**:\n",
            "   - Start from Mumbai and head towards the Mumbai-Pune Expressway. You can take the Eastern Express Highway or Western Express Highway to reach the expressway.\n",
            "\n",
            "2. **Kharghar Toll Plaza**:\n",
            "   - After around 35-40 kilometers on the expressway, you will come across the Kharghar Toll Plaza. Pay the toll and continue on the Mumbai-Pune Expressway.\n",
            "\n",
            "3. **Lonavala/Khandala Exit (optional)**:\n",
            "   - After driving for about 70-80 kilometers, you will see the exit for Lonavala/Khandala. If you want to take a short break or visit these popular hill stations, you can take this exit.\n",
            "\n",
            "4. **Talegaon Toll Plaza**:\n",
            "   - After crossing the Lonavala/Khandala exit, continue on the expressway for around 20 kilometers until you reach the Talegaon Toll Plaza. Pay the toll and proceed towards Pune.\n",
            "\n",
            "5. **Pune City Entry**:\n",
            "   - After passing the toll plaza, you will drive for another 30-40 kilometers before reaching Pune city. Follow the signs towards the city center.\n",
            "\n",
            "6. **Major Landmark - Pune University**:\n",
            "   - As you enter Pune, you may encounter the Pune University campus on your right. This serves as a major landmark indicating you are getting closer to the city center.\n",
            "\n",
            "7. **Destination - Pune City Center**:\n",
            "   - Once you are in Pune city, follow the signs towards your specific destination within the city center, whether it's a hotel, office, or any other location.\n",
            "\n",
            "Please note that this route is subject to traffic conditions, road closures, and diversions. It's a good idea to have a GPS device or map to navigate effectively. Also, make sure to have sufficient fuel and rest stops along the way for a comfortable journey. Drive safely and enjoy your trip from Mumbai to Pune!\n",
            "Route description exceeds 500 characters. Please provide a shorter request.\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "udio\n"
      ],
      "metadata": {
        "id": "GbQ4ySh8FT34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into chunks of approximately 500 characters\n",
        "    def chunks(lst, max_chunk_length):\n",
        "        chunk = []\n",
        "        current_length = 0\n",
        "        for sentence in lst:\n",
        "            sentence_length = len(sentence)\n",
        "            if current_length + sentence_length > max_chunk_length:\n",
        "                yield ' '.join(chunk)\n",
        "                chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "            else:\n",
        "                chunk.append(sentence)\n",
        "                current_length += sentence_length + 1  # Adding 1 for the space between sentences\n",
        "        if chunk:\n",
        "            yield ' '.join(chunk)\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', route_description)\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each chunk of sentences and join them back together\n",
        "    for chunk in chunks(sentences, 500):\n",
        "        translated_chunk = translator.translate(chunk)\n",
        "        translated_sentences.append(translated_chunk)\n",
        "\n",
        "    translated_description = ' '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio.export(\"temp.wav\", format=\"wav\")\n",
        "    with sr.AudioFile(\"temp.wav\") as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        text = recognizer.recognize_whisper(audio_data, model=\"base\")\n",
        "    return text\n",
        "\n",
        "# Run the chatbot\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students.\")\n",
        "while True:\n",
        "    input_type = input(\"Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): \").lower()\n",
        "\n",
        "    if input_type == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    elif input_type == 'text':\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "            continue\n",
        "\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "        target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "        translated_description = translate_description(route_description, target_language)\n",
        "        print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "    elif input_type == 'audio':\n",
        "        audio_path = input(\"Enter the path to the audio file: \")\n",
        "\n",
        "        try:\n",
        "            user_input = transcribe_audio(audio_path)\n",
        "            print(f\"Transcribed Text: {user_input}\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Invalid input format from audio transcription.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "            target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "            translated_description = translate_description(route_description, target_language)\n",
        "            print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'text' or 'audio'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swM84sXMFE_x",
        "outputId": "695d4193-12a0-46e7-afbf-918d4ea2c3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students.\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): text\n",
            "Enter your request: from mumbai to pune\n",
            "Route Description (in English): Sure! Here is a detailed driving route from Mumbai to Pune:\n",
            "\n",
            "1. Start in Mumbai and get on the Mumbai-Pune Expressway from the nearest entry point.\n",
            "2. Drive on the Mumbai-Pune Expressway for approximately 93 miles (150 km).\n",
            "3. Keep driving on the Expressway until you reach the Lonavala exit. Take the Lonavala exit to merge onto Old Mumbai Pune Highway/State Highway 54.\n",
            "4. Continue on the Old Mumbai Pune Highway for 6.5 miles (10.5 km) until you reach Lonavala town.\n",
            "5. In Lonavala, drive through the town on the Old Mumbai Pune Highway.\n",
            "6. As you exit Lonavala, continue on the Old Mumbai Pune Highway for another 26 miles (42 km) towards Pune.\n",
            "7. Pass through the towns of Kamshet and Talegaon Dabhade on the way to Pune.\n",
            "8. As you approach Pune, you will pass by Pune University and Pune IT Park on the left.\n",
            "9. Continue on the Old Mumbai Pune Highway until you reach the Pune city limits.\n",
            "10. Follow the signs towards your specific destination within Pune.\n",
            "\n",
            "Please note that the driving route and distances mentioned are approximate and may vary based on traffic conditions and road closures. It is recommended to use a GPS navigation system or map for real-time updates during your journey.\n",
            "Enter language code for translation (e.g., 'hi' for Hindi): hi\n",
            "Route Description (in hi): ज़रूर! मुंबई से पुणे के लिए ड्राइविंग का एक विस्तृत मार्ग यहाँ दिया गया है:\n",
            "\n",
            "1. मुंबई में शुरू करें और निकटतम प्रवेश बिंदु से मुंबई - पुणे एक्सप्रेसवे पर जाएं। 2. मुंबई - पुणे एक्सप्रेसवे पर लगभग 93 मील (150 किमी) तक ड्राइव करें। 3. लोनावाला से बाहर निकलने तक एक्सप्रेसवे पर गाड़ी चलाते रहें। पुराने मुंबई पुणे राजमार्ग/राज्य राजमार्ग 54 पर विलय करने के लिए लोनावाला से बाहर निकलें। 4. पुराने मुंबई पुणे राजमार्ग पर 6.5 मील (10.5 किमी) तक जारी रखें जब तक आप लोनावाला शहर तक नहीं पहुंच जाते। 5. लोनावाला में, पुराने मुंबई पुणे राजमार्ग पर शहर के माध्यम से ड्राइव करें। 6. जैसे ही आप लोनावाला से बाहर निकलते हैं, पुणे की ओर एक और 26 मील (42 किमी) के लिए पुराने मुंबई पुणे राजमार्ग पर जारी रखें। 7. पुणे के रास्ते में कामशेत और तालेगांव दाभाड़े के कस्बों से गुज़रें। 8. जैसे ही आप पुणे के पास पहुंचेंगे, आप बाईं ओर पुणे विश्वविद्यालय और पुणे आईटी पार्क से गुजरेंगे। 9. पुणे शहर की सीमा तक पहुंचने तक पुराने मुंबई पुणे राजमार्ग पर जारी रखें। 10. पुणे के भीतर अपने विशिष्ट डेस्टिनेशन की ओर संकेतों का पालन करें। कृपया ध्यान दें कि बताए गए ड्राइविंग रूट और दूरी अनुमानित हैं और ट्रैफ़िक की स्थिति और सड़क बंद होने के आधार पर अलग - अलग हो सकते हैं। अपनी यात्रा के दौरान रीयल - टाइम अपडेट के लिए जीपीएस नेविगेशन सिस्टम या मैप का इस्तेमाल करने की सलाह दी जाती है।\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): audio\n",
            "Enter the path to the audio file: /content/record1.in-speech.mp3\n",
            "Transcribed Text:  Provide route from Mumbai to Puna.\n",
            "Route Description (in English): Here is a detailed driving route from Mumbai to Pune:\n",
            "\n",
            "1. Start in Mumbai and head south on Dr. Annie Besant Road\n",
            "2. Take a slight left onto Lala Lajpat Rai Road and continue onto Sardar Vallabhbhai Patel Rd\n",
            "3. Merge onto Western Express Highway via the ramp to Mumbai Central Suburbs\n",
            "4. Continue on Western Express Highway for about 15 km\n",
            "5. Take the exit toward Mumbai - Agra National Highway/NH 3\n",
            "6. Merge onto Mumbai - Agra National Highway/NH 3\n",
            "7. Continue on NH 3 for about 100 km\n",
            "8. Take the slip road onto Pune - Nashik Highway/Mumbai - Agra National Highway/NH 50\n",
            "9. Continue on NH 50 for about 80 km\n",
            "10. Take the exit toward Wakad-Hinjewadi Road\n",
            "11. Merge onto Wakad-Hinjewadi Road and continue onto Hinjewadi Road\n",
            "12. Continue on Hinjewadi Road for about 10 km\n",
            "13. Turn right onto Baner Road\n",
            "14. Continue on Baner Road for about 8 km\n",
            "15. Turn left onto Mumbai-Pune Highway and continue onto Pune-Mumbai Expressway\n",
            "16. Continue on Pune-Mumbai Expressway for about 20 km\n",
            "17. Take the exit toward Pune\n",
            "18. Merge onto Pune Bangalore Highway/NH 4\n",
            "19. Follow signs for Pune and continue on NH 4 for about 10 km\n",
            "20. Turn left onto University Road\n",
            "21. Continue on University Road for about 5 km\n",
            "22. Turn right onto Ganeshkhind Road\n",
            "23. Continue on Ganeshkhind Road for about 2 km\n",
            "24. Turn left onto Shirole Road\n",
            "25. Continue on Shirole Road for about 1 km\n",
            "26. Turn right onto Ganeshkhind Road\n",
            "27. Continue on Ganeshkhind Road for about 1.5 km\n",
            "28. Turn left onto Ambedkar Road\n",
            "29. Continue on Ambedkar Road for about 2 km\n",
            "30. Arrive in Pune\n",
            "\n",
            "Total Distance: Approximately 150 km\n",
            "Estimated Driving Time: Around 3-4 hours, depending on traffic.\n",
            "Enter language code for translation (e.g., 'hi' for Hindi): hi\n",
            "Route Description (in hi): मुंबई से पुणे के लिए ड्राइविंग का एक विस्तृत मार्ग यहाँ दिया गया है:\n",
            "\n",
            "1. मुंबई में शुरू करें और डॉ. एनी बेसेंट रोड पर दक्षिण की ओर जाएं\n",
            "2. लाला लाजपत राय रोड पर थोड़ा बाएं ले जाएं और सरदार वल्लभभाई पटेल रोड पर जारी रखें\n",
            "3. मुंबई सेंट्रल सबर्ब के लिए रैंप के माध्यम से वेस्टर्न एक्सप्रेस हाईवे पर विलय करें\n",
            "4. लगभग 15 किमी के लिए वेस्टर्न एक्सप्रेस हाईवे पर जारी रखें\n",
            "5. मुंबई - आगरा राष्ट्रीय राजमार्ग/एनएच 3 की ओर निकास लें\n",
            "6. मुंबई - आगरा राष्ट्रीय राजमार्ग/एनएच 3 पर विलय करें\n",
            "7. लगभग 100 किमी के लिए NH 3 पर जारी रखें\n",
            "8. पुणे - नासिक राजमार्ग/मुंबई - आगरा राष्ट्रीय राजमार्ग/एनएच 50 पर स्लिप रोड लें\n",
            "9. NH 50 पर लगभग 80 किमी तक जारी रखें\n",
            "10. वकाद - हिंजेवाड़ी रोड की ओर निकास लें\n",
            "11. वकाद - हिंजेवाड़ी रोड पर विलय करें और हिंजेवाड़ी रोड पर जारी रखें\n",
            "12. हिंजेवाड़ी रोड पर लगभग 10 किमी तक जारी रखें\n",
            "13. बैनर रोड पर दाईं ओर मुड़ें\n",
            "14. बनर रोड पर लगभग 8 किमी तक जारी रखें\n",
            "15. बाएं मुड़ें मुंबई - पुणे राजमार्ग पर और पुणे - मुंबई एक्सप्रेसवे पर जारी रखें\n",
            "16. लगभग 20 किमी 17 के लिए पुणे - मुंबई एक्सप्रेसवे पर जारी रखें\n",
            "। पुणे की ओर निकलें\n",
            "18. पुणे बैंगलोर राजमार्ग/एनएच 4 पर विलय करें\n",
            "19. पुणे के लिए संकेतों का पालन करें और लगभग 10 किमी तक NH 4 पर जारी रखें\n",
            "20. यूनिवर्सिटी रोड पर बाईं ओर मुड़ें\n",
            "21. लगभग 5 किमी के लिए यूनिवर्सिटी रोड पर जारी रखें\n",
            "22. गणेशखिन्द रोड पर दाईं ओर मुड़ें\n",
            "23. गणेशखिन्द रोड पर लगभग 2 किमी तक जारी रखें\n",
            "24. शिरोले रोड पर बाएं मुड़ें\n",
            "25. लगभग 1 किमी तक शिरोले रोड पर जारी रखें\n",
            "26. गणेशखिन्द रोड पर दाईं ओर मुड़ें\n",
            "27. गणेशखिन्द रोड पर लगभग 1.5 किमी तक जारी रखें\n",
            "28. अम्बेडकर रोड 29 पर बाएं मुड़ें\n",
            "। अंबेडकर रोड पर लगभग 2 किमी तक जारी रखें\n",
            "30. पुणे पहुंचें\n",
            "\n",
            "कुल दूरी: लगभग 150 किमी\n",
            "गाड़ी चलाने का अनुमानित समय: ट्रैफ़िक के आधार पर लगभग 3 -4 घंटे।\n",
            "Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGNw-CTZGB2j",
        "outputId": "5ee75c9d-b622-4061-d5c2-bba5bc242677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "import pyaudio\n",
        "import wave\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into chunks of approximately 500 characters\n",
        "    def chunks(lst, max_chunk_length):\n",
        "        chunk = []\n",
        "        current_length = 0\n",
        "        for sentence in lst:\n",
        "            sentence_length = len(sentence)\n",
        "            if current_length + sentence_length > max_chunk_length:\n",
        "                yield ' '.join(chunk)\n",
        "                chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "            else:\n",
        "                chunk.append(sentence)\n",
        "                current_length += sentence_length + 1  # Adding 1 for the space between sentences\n",
        "        if chunk:\n",
        "            yield ' '.join(chunk)\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', route_description)\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each chunk of sentences and join them back together\n",
        "    for chunk in chunks(sentences, 500):\n",
        "        translated_chunk = translator.translate(chunk)\n",
        "        translated_sentences.append(translated_chunk)\n",
        "\n",
        "    translated_description = ' '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "def transcribe_audio():\n",
        "    # Initialize PyAudio\n",
        "    audio = pyaudio.PyAudio()\n",
        "\n",
        "    # Define parameters for audio recording\n",
        "    format = pyaudio.paInt16\n",
        "    channels = 1\n",
        "    rate = 16000  # Sample rate\n",
        "    chunk = 1024  # Size of each audio chunk\n",
        "\n",
        "    # Create an input stream\n",
        "    stream = audio.open(format=format,\n",
        "                        channels=channels,\n",
        "                        rate=rate,\n",
        "                        input=True,\n",
        "                        frames_per_buffer=chunk)\n",
        "\n",
        "    print(\"Recording audio...\")\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    # Record audio in chunks and save to frames list\n",
        "    while True:\n",
        "        try:\n",
        "            data = stream.read(chunk)\n",
        "            frames.append(data)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Recording stopped.\")\n",
        "            break\n",
        "\n",
        "    # Stop and close the audio stream\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    audio.terminate()\n",
        "\n",
        "    # Save the recorded audio to a WAV file\n",
        "    wave_output_file = \"recorded_audio.wav\"\n",
        "    with wave.open(wave_output_file, 'wb') as wf:\n",
        "        wf.setnchannels(channels)\n",
        "        wf.setsampwidth(audio.get_sample_size(format))\n",
        "        wf.setframerate(rate)\n",
        "        wf.writeframes(b''.join(frames))\n",
        "\n",
        "    return wave_output_file\n",
        "\n",
        "# Run the chatbot\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students.\")\n",
        "while True:\n",
        "    input_type = input(\"Enter 'text' for text input or 'audio' for audio input (type 'exit' to quit): \").lower()\n",
        "\n",
        "    if input_type == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    elif input_type == 'text':\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "            continue\n",
        "\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "        target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "        translated_description = translate_description(route_description, target_language)\n",
        "        print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "    elif input_type == 'audio':\n",
        "        try:\n",
        "            audio_file = transcribe_audio()\n",
        "            print(f\"Audio recording saved to: {audio_file}\")\n",
        "\n",
        "            # Perform transcription and further processing with the recorded audio\n",
        "            user_input = transcribe_audio(audio_file)\n",
        "            print(f\"Transcribed Text: {user_input}\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Invalid input format from audio transcription.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "            target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "            translated_description = translate_description(route_description, target_language)\n",
        "            print(f\"Route Description (in {target_language}):\", translated_description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'text' or 'audio'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fUw4rPqlGJKC",
        "outputId": "0d98e8e3-aacf-457a-f522-ba1ff6caac9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyaudio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-728c053766e8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtflyC64Cuyd",
        "outputId": "288a30a7-7d55-40f2-8f1f-7d2cd7778dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_MXhctuCKzO",
        "outputId": "79be4f50-e804-4a7a-abc5-722ccdee159d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "from translate import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expressions to extract locations from input\n",
        "    patterns = [\n",
        "        re.compile(r\"take me from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE),\n",
        "        re.compile(r\"i want to go from\\s+(.+?)\\s+to\\s+(.+)\", re.IGNORECASE)\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(user_input)\n",
        "        if match:\n",
        "            start_location = match.group(1).strip()\n",
        "            end_location = match.group(2).strip()\n",
        "            return start_location, end_location\n",
        "    return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "def translate_description(route_description, target_language):\n",
        "    if target_language == 'en':\n",
        "        return route_description  # No translation needed if English is selected\n",
        "\n",
        "    # Initialize translator\n",
        "    translator = Translator(to_lang=target_language)\n",
        "\n",
        "    # Split route description into manageable chunks (e.g., sentences)\n",
        "    sentences = route_description.split('. ')\n",
        "    translated_sentences = []\n",
        "\n",
        "    # Translate each sentence and join them back together\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = translator.translate(sentence)\n",
        "        translated_sentences.append(translated_sentence)\n",
        "\n",
        "    translated_description = '. '.join(translated_sentences)\n",
        "\n",
        "    return translated_description\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"Enter your request: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use phrases like 'take me from [start location] to [end location]', 'from [start location] to [end location]', or 'I want to go from [start location] to [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description (in English):\", route_description)\n",
        "\n",
        "    # Ask user for desired language for route description\n",
        "    target_language = input(\"Enter language code for translation (e.g., 'hi' for Hindi): \").lower()\n",
        "\n",
        "    translated_description = translate_description(route_description, target_language)\n",
        "    print(f\"Route Description (in {target_language}):\",translated_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0OCDnjYz_ErO",
        "outputId": "88dde80f-b78b-4b78-a0a1-a14d4321556b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6f5cf5bff209>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Greetings! I am SARATHI, developed by NIT Calicut students. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \"\"\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to chunk the messages\n",
        "def chunk_messages(messages, max_tokens=1500):\n",
        "    chunked_messages = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for message in messages:\n",
        "        if len(\" \".join(current_chunk)) + len(message['content']) <= max_tokens:\n",
        "            current_chunk.append(message)\n",
        "        else:\n",
        "            chunked_messages.append(current_chunk)\n",
        "            current_chunk = [message]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunked_messages.append(current_chunk)\n",
        "\n",
        "    return chunked_messages\n",
        "\n",
        "# Prepare training data\n",
        "# Each training example should be a separate message object\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "for _, row in data.iterrows():\n",
        "    prompt = (\n",
        "        f\"Issue Title: {row['title']}\\n\"\n",
        "        f\"Issue Body: {row['body']}\\n\"\n",
        "        f\"Issue Labels: {row['labels']}\\n\"\n",
        "        f\"Issue URL: {row['issue_url']}\\n\"\n",
        "        f\"Repository URL: {row['repository_url']}\\n\"\n",
        "        f\"Language: {row['language']}\\n\"\n",
        "        \"Response:\"\n",
        "    )\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})  # Add each prompt as a user message\n",
        "\n",
        "# Chunk the messages\n",
        "chunked_messages = chunk_messages(messages)\n",
        "\n",
        "# Train the language model\n",
        "# Pass each chunk of messages to the API sequentially\n",
        "for chunk in chunked_messages:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=chunk,\n",
        "        max_tokens=1500,  # Adjust as per your needs\n",
        "        temperature=0.7,  # Adjust as per your needs\n",
        "        stop=[\"\\n\"]\n",
        "    )\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "pbyjQ6O2K33q",
        "outputId": "29950ded-2f65-4dc8-8d50-90c7cdd40c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sequence item 0: expected str instance, dict found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a9aa33f659f2>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Chunk the messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mchunked_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Train the language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a9aa33f659f2>\u001b[0m in \u001b[0;36mchunk_messages\u001b[0;34m(messages, max_tokens)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mcurrent_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \"\"\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare training data\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "for _, row in data.iterrows():\n",
        "    prompt = (\n",
        "        f\"Issue Title: {row['title']}\\n\"\n",
        "        f\"Issue Body: {row['body']}\\n\"\n",
        "        f\"Issue Labels: {row['labels']}\\n\"\n",
        "        f\"Issue URL: {row['issue_url']}\\n\"\n",
        "        f\"Repository URL: {row['repository_url']}\\n\"\n",
        "        f\"Language: {row['language']}\\n\"\n",
        "        \"Response:\"\n",
        "    )\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})  # Add each prompt as a user message\n",
        "\n",
        "# Function to chunk the messages\n",
        "def chunk_messages(messages, max_tokens=1500):\n",
        "    chunked_messages = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for message in messages:\n",
        "        if len(\" \".join(current_chunk)) + len(message['content']) <= max_tokens:\n",
        "            current_chunk.append(message)\n",
        "        else:\n",
        "            chunked_messages.append(current_chunk)\n",
        "            current_chunk = [message]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunked_messages.append(current_chunk)\n",
        "\n",
        "    return chunked_messages\n",
        "\n",
        "# Chunk the messages\n",
        "chunked_messages = chunk_messages(messages)\n",
        "\n",
        "# Train the language model (gpt-4)\n",
        "for chunk in chunked_messages:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"text-davinci-002\",\n",
        "        messages=chunk,\n",
        "        max_tokens=1500,  # Adjust as per your needs\n",
        "        temperature=0.7,  # Adjust as per your needs\n",
        "        stop=[\"\\n\"]\n",
        "    )\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "BmkYjyLPMrNE",
        "outputId": "bf1735e7-9df4-4c3e-fc6c-f544fc586219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sequence item 0: expected str instance, dict found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-10a0eb83d712>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Chunk the messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mchunked_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Train the language model (gpt-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-10a0eb83d712>\u001b[0m in \u001b[0;36mchunk_messages\u001b[0;34m(messages, max_tokens)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mcurrent_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHvCnxbkNAfd",
        "outputId": "59997349-e599-46b4-b6c3-1b9fc28d01c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Set your OpenAI API key if needed\n",
        "api_key = \"\"\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare training data\n",
        "messages = []\n",
        "for _, row in data.iterrows():\n",
        "    prompt = (\n",
        "        f\"Issue Title: {row['title']}\\n\"\n",
        "        f\"Issue Body: {row['body']}\\n\"\n",
        "        f\"Issue Labels: {row['labels']}\\n\"\n",
        "        f\"Issue URL: {row['issue_url']}\\n\"\n",
        "        f\"Repository URL: {row['repository_url']}\\n\"\n",
        "        f\"Language: {row['language']}\\n\"\n",
        "        \"Response:\"\n",
        "    )\n",
        "    messages.append(prompt)\n",
        "\n",
        "# Initialize GPT-Neo tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "\n",
        "# Tokenize the training data\n",
        "tokenized_datasets = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    texts=messages,\n",
        "    block_size=128  # Adjust block size as per your needs\n",
        ")\n",
        "\n",
        "# Prepare data collator and training arguments\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # Adjust batch size as per your GPU memory\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    overwrite_output_dir=True,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Trainer to train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0DU93AQxNDVD",
        "outputId": "03a52208-3dce-4c7b-de38-226ac96545c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TextDataset.__init__() got an unexpected keyword argument 'texts'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6f0805930fc1>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Tokenize the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m tokenized_datasets = TextDataset(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextDataset.__init__() got an unexpected keyword argument 'texts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iko5Zr_Psbf",
        "outputId": "ce954b64-523d-43ea-e004-6cf181830566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Set your OpenAI API key if needed\n",
        "api_key = \"\"\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare training data\n",
        "dataset = Dataset.from_pandas(data)\n",
        "\n",
        "# Initialize GPT-Neo tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "\n",
        "# Tokenize the training data\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize title and body separately\n",
        "    title_tokens = tokenizer(examples[\"title\"], return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length')\n",
        "    body_tokens = tokenizer(examples[\"body\"], return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length')\n",
        "\n",
        "    # Combine the tokenized inputs\n",
        "    return {\n",
        "        'input_ids': torch.cat((title_tokens['input_ids'], body_tokens['input_ids']), dim=1),\n",
        "        'attention_mask': torch.cat((title_tokens['attention_mask'], body_tokens['attention_mask']), dim=1)\n",
        "    }\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare data collator and training arguments\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # Adjust batch size as per your GPU memory\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    overwrite_output_dir=True,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Trainer to train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "id": "8QdP5u77Pt_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pandas as pd\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Load your dataset (assuming it's already cleaned and prepared)\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize the language model (assuming OpenAI GPT-3.5)\n",
        "llm = OpenAI()\n",
        "\n",
        "# Define a prompt template for question asking\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=(\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Response:\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create LLMChain for conversational bot\n",
        "question_answering_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Example interaction loop with the chatbot\n",
        "print(\"Welcome to the Dataset Question Bot!\")\n",
        "print(\"You can ask questions about the dataset or type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask a question or type 'exit': \").strip()\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting...')\n",
        "        break\n",
        "\n",
        "    # Prepare user query\n",
        "    user_query = {\n",
        "        \"question\": user_input.strip()\n",
        "    }\n",
        "\n",
        "    # Get response from the chatbot\n",
        "    response = question_answering_chain.run(user_query)\n",
        "\n",
        "    print(\"Bot Response:\")\n",
        "    print(response)\n",
        "    print(\"-------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "UWymX9FGU_5p",
        "outputId": "57eadbc5-b219-4672-ea3d-2e024a726d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Dataset Question Bot!\n",
            "You can ask questions about the dataset or type 'exit' to quit.\n",
            "Ask a question or type 'exit': provide details about optimised images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot Response:\n",
            " Optimized images are digital images that have been optimized for web use to reduce their file size without compromising their quality. This is done by compressing the image file, removing unnecessary metadata, and resizing the image to the optimal dimensions for web display. Optimized images are important for improving website performance, as they load faster and use less bandwidth. This leads to a better user experience and can also improve search engine rankings. There are various tools and techniques available to optimize images, such as using file formats like JPEG, PNG, or WebP, using compression software, and optimizing image dimensions. By using optimized images, website owners can provide a visually appealing experience to their users while also improving website speed and performance.\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0cca4e8a753a>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question or type 'exit': \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "IV4y8DIxWkOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "s1oEk_QDWsNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke(\"what issues were faced in optimised images?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "X_QVSmvjXHbs",
        "outputId": "280a957a-7258-4dc7-c601-f7516be5557f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent_executer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a175702368c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_executer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what issues were faced in optimised images?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent_executer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.5)\n",
        "agent_executer = create_csv_agent(llm, '/content/cleaned_first_100_issues.csv', verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "CE8eOLiYXav_",
        "outputId": "20c1cb6a-340a-4a38-8678-abffb7707db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'OpenAI'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f69dc6dd0ac7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent_executer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_csv_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/cleaned_first_100_issues.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROOT_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             values[\"client\"] = openai.OpenAI(\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             ).chat.completions\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import faiss\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "On-3Si2EX0AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file containing question-answer pairs and store it in a DataFrame called qa\n",
        "qa = pd.read_csv(\"/content/cleaned_first_100_issues.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to ensure data has been read properly\n",
        "qa.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SoDjPhcRYKyV",
        "outputId": "c40a9e58-4d6d-47e5-9d45-75828563f7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        title  \\\n",
              "0  Added python code example for parking lot    \n",
              "1                            Optimised Images   \n",
              "2                         de: Add translation   \n",
              "3                               small changes   \n",
              "4                 ru: Add Russian translation   \n",
              "\n",
              "                                              labels  \\\n",
              "0  [{\"name:\": \"needs-review\", \"description:\": \"no...   \n",
              "1  [{\"name:\": \"needs-review\", \"description:\": \"no...   \n",
              "2  [{\"name:\": \"needs-review\", \"description:\": \"no...   \n",
              "3  [{\"name:\": \"needs-review\", \"description:\": \"no...   \n",
              "4  [{\"name:\": \"needs-review\", \"description:\": \"no...   \n",
              "\n",
              "                                                body  \\\n",
              "0  …sign\\n\\n## Review the Contributing Guidelines...   \n",
              "1  ## Review the Contributing Guidelines\\n\\nBefor...   \n",
              "2  ## Review the Contributing Guidelines\\n\\nBefor...   \n",
              "3  ## Review the Contributing Guidelines\\n\\nBefor...   \n",
              "4  * Added README-ru.md\\n* Kept content of origin...   \n",
              "\n",
              "                                           issue_url  \\\n",
              "0  https://api.github.com/repos/donnemartin/syste...   \n",
              "1  https://api.github.com/repos/donnemartin/syste...   \n",
              "2  https://api.github.com/repos/donnemartin/syste...   \n",
              "3  https://api.github.com/repos/donnemartin/syste...   \n",
              "4  https://api.github.com/repos/donnemartin/syste...   \n",
              "\n",
              "                                      repository_url language  \n",
              "0  https://api.github.com/repos/donnemartin/syste...       en  \n",
              "1  https://api.github.com/repos/donnemartin/syste...       en  \n",
              "2  https://api.github.com/repos/donnemartin/syste...       en  \n",
              "3  https://api.github.com/repos/donnemartin/syste...       en  \n",
              "4  https://api.github.com/repos/donnemartin/syste...       en  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1a23b25-6325-448c-a4a3-ec3c2f9b30b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>labels</th>\n",
              "      <th>body</th>\n",
              "      <th>issue_url</th>\n",
              "      <th>repository_url</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Added python code example for parking lot</td>\n",
              "      <td>[{\"name:\": \"needs-review\", \"description:\": \"no...</td>\n",
              "      <td>…sign\\n\\n## Review the Contributing Guidelines...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Optimised Images</td>\n",
              "      <td>[{\"name:\": \"needs-review\", \"description:\": \"no...</td>\n",
              "      <td>## Review the Contributing Guidelines\\n\\nBefor...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>de: Add translation</td>\n",
              "      <td>[{\"name:\": \"needs-review\", \"description:\": \"no...</td>\n",
              "      <td>## Review the Contributing Guidelines\\n\\nBefor...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>small changes</td>\n",
              "      <td>[{\"name:\": \"needs-review\", \"description:\": \"no...</td>\n",
              "      <td>## Review the Contributing Guidelines\\n\\nBefor...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ru: Add Russian translation</td>\n",
              "      <td>[{\"name:\": \"needs-review\", \"description:\": \"no...</td>\n",
              "      <td>* Added README-ru.md\\n* Kept content of origin...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>https://api.github.com/repos/donnemartin/syste...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a23b25-6325-448c-a4a3-ec3c2f9b30b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1a23b25-6325-448c-a4a3-ec3c2f9b30b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1a23b25-6325-448c-a4a3-ec3c2f9b30b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0259087f-f32b-4841-bb7d-e9fa1cef9847\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0259087f-f32b-4841-bb7d-e9fa1cef9847')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0259087f-f32b-4841-bb7d-e9fa1cef9847 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "qa",
              "summary": "{\n  \"name\": \"qa\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Added detail on CAP theorem limitations\",\n          \"Added scale cube\",\n          \"ja: Correct mistranslations in Communication section\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"[{\\\"name:\\\": \\\"needs-review\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"5319e7\\\"}, {\\\"name:\\\": \\\"response-needed\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"207de5\\\"}]\",\n          \"[{\\\"name:\\\": \\\"incorporating-feedback\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"006b75\\\"}, {\\\"name:\\\": \\\"response-needed\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"207de5\\\"}, {\\\"name:\\\": \\\"translation\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"f9d0c4\\\"}]\",\n          \"[{\\\"name:\\\": \\\"needs-review\\\", \\\"description:\\\": \\\"none\\\", \\\"color\\\": \\\"5319e7\\\"}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 84,\n        \"samples\": [\n          \"Browsing through the solution to **[Design a parking lot](https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/parking_lot/parking_lot.py)**, it seems the current solution is not addressing the some constraints, and are missing some implementation\\n1. bus need to take 5 consecutive spots. \\n   a. `Bus.can_fit_in_spot` not checking number of available spots\\n   b. no consecutive spots check available in `ParkingSpot`\\n2. `Level.spot_freed` always release 1 spot, not considering bus taking 5 spots. Perhaps this should take a `Vehicle` object as argument and release corresponding spots\\n3. `Level._find_available_spot` and `Level._park_starting_at_spot` not implemented.\\n4. `ParkingSpot.park_vehicle` and `ParkingSpot.remove_vehicle` not implemented.\\n\\nI can submit a PR if maintainers think this is a valid enhancement.\\n\",\n          \"\\u2026sign\\n\\n## Review the Contributing Guidelines\\n\\nBefore submitting a pull request, verify it meets all requirements in the [Contributing Guidelines](https://github.com/donnemartin/system-design-primer/blob/master/CONTRIBUTING.md).\\n\\n### Translations\\n\\nSee the [Contributing Guidelines](https://github.com/donnemartin/system-design-primer/blob/master/CONTRIBUTING.md).  Verify you've:\\n\\n* Tagged the [language maintainer](https://github.com/donnemartin/system-design-primer/blob/master/TRANSLATIONS.md)\\n* Prefixed the title with a language code\\n    * Example: \\\"ja: Fix ...\\\"\\n\",\n          \"* `\\u7d50\\u9ede` -> `\\u7bc0\\u9ede`\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"issue_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"https://api.github.com/repos/donnemartin/system-design-primer/issues/297\",\n          \"https://api.github.com/repos/donnemartin/system-design-primer/issues/413\",\n          \"https://api.github.com/repos/donnemartin/system-design-primer/issues/331\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"repository_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"https://api.github.com/repos/donnemartin/system-design-primer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import AgentExecutor, initialize_agent\n",
        "\n",
        "# Initialize OpenAI for embedding (if needed) and chat capabilities\n",
        "openai_agent = initialize_agent(ChatOpenAI(api_key=\"\"))\n",
        "\n",
        "# Initialize conversation memory\n",
        "memory = ConversationBufferWindowMemory(window_size=5)  # Adjust window_size as needed\n",
        "\n",
        "# Initialize AgentExecutor\n",
        "agent_executor = AgentExecutor(agent=openai_agent, memory=memory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZtaCUaJPYobY",
        "outputId": "d14b23f2-8ccf-4d7e-f62f-0615fbd69127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'OpenAI'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d1aba3967fa3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize OpenAI for embedding (if needed) and chat capabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mopenai_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize conversation memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROOT_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             values[\"client\"] = openai.OpenAI(\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             ).chat.completions\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -q langchain openai chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHXl65TTa9qk",
        "outputId": "d5e60fb1-8bb8-4821-a2b3-36364a0c82ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "UlFENYdtbLOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "ilEpfNOabRWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path='/content/cleaned_first_100_issues.csv')"
      ],
      "metadata": {
        "id": "LY36qCBvbZLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an index using the loaded documents\n",
        "index_creator = VectorstoreIndexCreator()\n",
        "docsearch = index_creator.from_loaders([loader])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "RCcKCiDkbivn",
        "outputId": "eb29a4c6-42f5-4f3e-ce79-84e30c5d8760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/indexes/vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for VectorstoreIndexCreator\nembedding\n  field required (type=value_error.missing)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d4743758fabe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create an index using the loaded documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorstoreIndexCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdocsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for VectorstoreIndexCreator\nembedding\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai langchain tqdm python-dotenv pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr6Rap9uYWxN",
        "outputId": "76c060fb-943f-40ae-a287-cfad34ceae79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.9)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.35.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain-openai) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain-openai) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zEHlOh7X3hY",
        "outputId": "c9e6408c-80e9-4498-b2a6-2ed4beb01ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_experimental\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyZRf3KtWwDv",
        "outputId": "5521b9f8-14e0-47c1-9e13-2d5e3c55b1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.10/dist-packages (0.0.61)\n",
            "Requirement already satisfied: langchain-community<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.2.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.2.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.1.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.5->langchain_experimental) (8.4.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.7.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (0.2.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.7->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.5->langchain_experimental) (1.0.0)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.9)\n",
            "Collecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n",
            "  Using cached openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.81)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: openai, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed langchain_openai-0.1.8 openai-1.35.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtZEZMiKW1A2",
        "outputId": "e37262d1-3413-481c-ddeb-13b2e3212a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 faiss-cpu-1.8.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine the columns to create a single input text for training\n",
        "data['input_text'] = data.apply(lambda row: f\"Issue Title: {row['title']}\\nIssue Body: {row['body']}\\nIssue Labels: {row['labels']}\\nIssue URL: {row['issue_url']}\\nRepository URL: {row['repository_url']}\\nLanguage: {row['language']}\", axis=1)\n",
        "data['output_text'] = data['body']  # Assuming you want the body as the output text\n",
        "\n",
        "# Prepare the dataset for Hugging Face\n",
        "hf_dataset = Dataset.from_pandas(data[['input_text', 'output_text']])\n"
      ],
      "metadata": {
        "id": "aBWIY4Zh55hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=1024)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['input_text']\n",
        "    targets = examples['output_text']\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding='max_length')\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        # Convert targets to a list of strings\n",
        "        labels = tokenizer(targets.tolist(), max_length=1024, truncation=True, padding='max_length')\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "71ac778448954e9da5b1dd8af0b7670e",
            "bc2d2784e6f4462b90a29c40c46eedf9",
            "3623920d1da44a40a5ee8ddd2ac95a22",
            "82364729062345b08013748b095bb279",
            "728492e96efb47a98ae94c88aa35cef1",
            "9e2bfb0f16f94814ba898b8f37b6a51d",
            "27484e6e16a54438934773b2c67756bb",
            "db6f77b19a0e465699e638ee28f8e27d",
            "8769b9b6b3b947a089225c62e3f7e7fc",
            "ae77e0cfc76a4b14adc60d4669580fe9",
            "a530b98f59914d63ba0e1387a1898489"
          ]
        },
        "id": "Ey9L78IJ_tvx",
        "outputId": "ba1849b0-b433-4a15-9ed4-bd04108582ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71ac778448954e9da5b1dd8af0b7670e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'tolist'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0ba806078e9d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         }\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3159\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3161\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3550\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3551\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3552\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3553\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3422\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3423\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-27-0ba806078e9d>\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_target_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Convert targets to a list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine the columns to create a single input text for training\n",
        "data['input_text'] = data.apply(lambda row: f\"Issue Title: {row['title']}\\nIssue Body: {row['body']}\\nIssue Labels: {row['labels']}\\nIssue URL: {row['issue_url']}\\nRepository URL: {row['repository_url']}\\nLanguage: {row['language']}\", axis=1)\n",
        "data['output_text'] = data['body']  # Assuming you want the body as the output text\n",
        "\n",
        "# Prepare the dataset for Hugging Face\n",
        "hf_dataset = Dataset.from_pandas(data[['input_text', 'output_text']])\n",
        "\n",
        "# Load the tokenizer\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=1024)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['input_text']\n",
        "    targets = examples['output_text']\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(text_target=targets, max_length=1024, truncation=True, padding='max_length')\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./BARTModel_for_github\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=15,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,  # Here we use the same dataset for simplicity\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./fine-tuned-bart\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bart\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "58e1211da2a242249473419cbae795fd",
            "a25b0c5156304beaacd1c6aed79584c5",
            "c2ddf716acdf49b8b4c4e5e077353b9c",
            "d237091bbfd1491785de8406766e42b1",
            "8c36d3f7e65d4c9483c272a52b9291df",
            "12c8a34bfd2444629a76d544e8e499c8",
            "5b2b4bd84d134b1482adc79496a67c7c",
            "ecedc96e5b8c4bc5bb690e3363dd750a",
            "0338835ee8794877bac77c62ffbf66ac",
            "6ec64f0c4d5e4e0989e0f66ad44c5147",
            "d7d16493e16d41a6b1011ffe431080db"
          ]
        },
        "id": "FgenJ_7KAcWw",
        "outputId": "910b953a-b567-4ed1-941f-bfa5f473181e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58e1211da2a242249473419cbae795fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-31020e722af8>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         }\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3159\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3161\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3550\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3551\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3552\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3553\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3422\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3423\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-29-31020e722af8>\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2886\u001b[0;31m             \u001b[0mtarget_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2887\u001b[0m         \u001b[0;31m# Leave back tokenizer in input mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2888\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m                 )\n\u001b[1;32m   2968\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2970\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         )\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3160\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3161\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/tokenization_bart_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m             )\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    509\u001b[0m         )\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add a padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5f268e40ea534c43843b0fe3bb9b5744",
            "05095e1df8ba4c4489d375455547ec13",
            "ca439f97a0e5436c96980917cce5d991",
            "4f72f03953b84cd899d7367898f1683e",
            "5027f883d9fe490b96bf7364918f8720",
            "2d2f85781a2649d4a4732344b98586c2",
            "cda3abc382df49f8b1914956084b1433",
            "4bc1897b3d0542e99968bf57c3fdf7c5",
            "a353acd85414475d8e0c0292f88ab6a5",
            "1c9190b4bf134678864e63cfef29e666",
            "1d7ddad8295d4a19a536556e6abe971a"
          ]
        },
        "id": "QYMM21Od7Sd2",
        "outputId": "ce7ef7ac-9acf-4512-d406-c9f7cc694b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f268e40ea534c43843b0fe3bb9b5744"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Resize token embeddings to accommodate the new pad token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    warmup_steps=500,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,  # Here we use the same dataset for simplicity\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "stouYLh87yJg",
        "outputId": "25600963-689c-4226-daae-4b8efd821354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2d406046fe78>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length,...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_greater_or_equal_than_2_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mlu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \"\"\"\n\u001b[1;32m   2148\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2055\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2056\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine the columns to create a single input text for training\n",
        "data['input_text'] = data.apply(lambda row: f\"Issue Title: {row['title']}\\nIssue Body: {row['body']}\\nIssue Labels: {row['labels']}\\nIssue URL: {row['issue_url']}\\nRepository URL: {row['repository_url']}\\nLanguage: {row['language']}\", axis=1)\n",
        "data['output_text'] = data['body']  # Assuming you want the body as the output text\n",
        "\n",
        "# Custom Dataset Class\n",
        "class GithubIssuesDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.data.iloc[index]['input_text']\n",
        "        output_text = self.data.iloc[index]['output_text']\n",
        "\n",
        "        inputs = self.tokenizer(input_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.tokenizer(output_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "        labels = outputs['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
        "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = GithubIssuesDataset(data, tokenizer, max_len=1024)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}/{3}\")\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./fine-tuned-bart\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bart\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "e5496f480625408a9a971034567590bf",
            "4324df90bce845e59d8061371bd0b23f",
            "b4722b0023d740b085402bcfda7a6e37",
            "f743718474bf4d4a8167749419673a0b",
            "0c4799d78d804d028619ca2a0047be08",
            "c66a2a39d8bb4beba160369d865d1cb8",
            "4aea68295a0a4ab691a0e8dc6e4add37",
            "8cbb8a3c382a4e9587b4849ef8d96cd3",
            "dd43c4e7a30c4fe1af0f0a959f61a6a7",
            "ad72949e50ad46dda2b9248b1ee18b6d",
            "3736f14255b44e6ab886c191f3e5542e"
          ]
        },
        "id": "GmNAv1_vFxEg",
        "outputId": "d6691dc4-68cf-444a-d123-dce862a1f5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5496f480625408a9a971034567590bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.07 GiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-fb828de88660>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1758\u001b[0m         )\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.07 GiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine the columns to create a single input text for training\n",
        "data['input_text'] = data.apply(lambda row: f\"Issue Title: {row['title']}\\nIssue Body: {row['body']}\\nIssue Labels: {row['labels']}\\nIssue URL: {row['issue_url']}\\nRepository URL: {row['repository_url']}\\nLanguage: {row['language']}\", axis=1)\n",
        "data['output_text'] = data['body']  # Assuming you want the body as the output text\n",
        "\n",
        "# Custom Dataset Class\n",
        "class GithubIssuesDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.data.iloc[index]['input_text']\n",
        "        output_text = self.data.iloc[index]['output_text']\n",
        "\n",
        "        inputs = self.tokenizer(input_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.tokenizer(output_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "        labels = outputs['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
        "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = GithubIssuesDataset(data, tokenizer, max_len=1024)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Gradient accumulation steps\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}/{3}\")\n",
        "    total_loss = 0.0\n",
        "    for step, batch in enumerate(tqdm(train_loader)):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average loss: {avg_loss}\")\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./fine-tuned-bart\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bart\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "674b179f334547c6a07985466b7e0f76",
            "b0baf98ff48e49a78abdc209e25b7232",
            "a6b4bd3e91e544c7a8e029654ff1bd2b",
            "40eba478c3344456906f903a70cf7d57",
            "faaefa7572014e279a5a5827d2d35788",
            "7a1e6de288964985ad3fbf631fa3500c",
            "bba79497b99345e0b2628a42811410b9",
            "cf9f572be07c4567be336b0978c285f1",
            "c5e98504ad0d487c9751ed1a95d8aa58",
            "55475dac93c842c0a3e0072483922b2e",
            "cac8a68c63bc472fb7ea657d1abbd8de"
          ]
        },
        "id": "ZlRK80sxGT52",
        "outputId": "709465fe-d100-4c3d-934b-0f02d0e50e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "674b179f334547c6a07985466b7e0f76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a6b844aa9bf0>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                 )\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1743\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1629\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 )\n\u001b[1;32m   1480\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1482\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/cleaned_first_100_issues.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine the columns to create a single input text for training\n",
        "data['input_text'] = data.apply(lambda row: f\"Issue Title: {row['title']}\\nIssue Body: {row['body']}\\nIssue Labels: {row['labels']}\\nIssue URL: {row['issue_url']}\\nRepository URL: {row['repository_url']}\\nLanguage: {row['language']}\", axis=1)\n",
        "data['output_text'] = data['body']  # Assuming you want the body as the output text\n",
        "\n",
        "# Custom Dataset Class\n",
        "class GithubIssuesDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_text = self.data.iloc[index]['input_text']\n",
        "        output_text = self.data.iloc[index]['output_text']\n",
        "\n",
        "        inputs = self.tokenizer(input_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.tokenizer(output_text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "        labels = outputs['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
        "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = GithubIssuesDataset(data, tokenizer, max_len=1024)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, pin_memory=True)  # Enable pin_memory for faster data transfer\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Training loop with mixed precision\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "scaler = GradScaler()  # Initialize GradScaler for mixed precision training\n",
        "\n",
        "# Gradient accumulation steps\n",
        "gradient_accumulation_steps = 16  # Increased gradient accumulation\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}/3\")\n",
        "    total_loss = 0.0\n",
        "    for step, batch in enumerate(tqdm(train_loader)):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast():  # Enable autocast for mixed precision training\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Delete unnecessary tensors to free up memory\n",
        "        del input_ids, attention_mask, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average loss: {avg_loss}\")\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./fine-tuned-bart\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-bart\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "HzT7RH-7GqGu",
        "outputId": "a0c0f879-2b13-458e-a7e5-57f850a552ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9203ad82a079>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Training loop with mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize GradScaler for mixed precision training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m                 )\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-> 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVu8LWqL9efu",
        "outputId": "82c87c24-296e-44e7-9e8d-439c915eacbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=1024)  # Set model_max_length\n",
        "\n",
        "tokenized_Data = data.map(preprocess_function, batched=True)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"BARTModel_for_github\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=15,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "0UDvLmJV9FKr",
        "outputId": "f6c56d2c-ce08-4527-9222-97049150cce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-058f18a37e6a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenized_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Define your preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['input_text']\n",
        "    targets = examples['output_text']\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=1024, truncation=True, padding=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the DataFrame\n",
        "tokenized_data = preprocess_function(data.to_dict(orient='list'))\n",
        "\n",
        "# Convert to a Dataset object\n",
        "from datasets import Dataset\n",
        "tokenized_dataset = Dataset.from_dict(tokenized_data)\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# ... (Your existing code for model and training arguments)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    # eval_dataset=tokenized_eval_dataset if tokenized_eval_dataset else None,\n",
        "    data_collator=data_collator,\n",
        "    # ... other arguments\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "QkegCfARJRAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./fine-tuned-gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-gpt2\")\n"
      ],
      "metadata": {
        "id": "BAJSLEZS7431"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXFQuirH8CKx",
        "outputId": "4233ecb2-9605-46f3-e9f2-9fa8bcbe26d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/309.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m225.3/309.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key =\n",
        "openai.api_key = api_key\n",
        "\n",
        "# Initialize the language model\n",
        "llm = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Function to chunk the text into segments\n",
        "def chunk_text(text, max_tokens=4000):\n",
        "    # Split the text into chunks based on max_tokens\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        if current_length + len(token) <= max_tokens:\n",
        "            current_chunk.append(token)\n",
        "            current_length += len(token) + 1  # Add 1 for space\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [token]\n",
        "            current_length = len(token) + 1  # Add 1 for space\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to interact with the chatbot\n",
        "def chat_with_bot(user_query):\n",
        "    # Chunk the user query into smaller parts\n",
        "    query_chunks = chunk_text(user_query)\n",
        "\n",
        "    # Initialize list to store responses\n",
        "    responses = []\n",
        "\n",
        "    # Iterate over chunks and generate responses\n",
        "    for chunk in query_chunks:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": chunk}\n",
        "            ],\n",
        "            max_tokens=150,  # Adjust as per your needs\n",
        "            temperature=0.7,  # Adjust as per your needs\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        responses.append(response.choices[0].message['content'].strip())\n",
        "\n",
        "    # Combine all responses into a single response\n",
        "    combined_response = \"\\n\".join(responses)\n",
        "\n",
        "    return combined_response\n",
        "\n",
        "# Example usage\n",
        "while True:\n",
        "    user_input = input(\"Enter your query (type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    # Call the function to interact with the chatbot\n",
        "    response = chat_with_bot(user_input)\n",
        "    print(\"Response:\")\n",
        "    print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "1hTMF1qOL_6K",
        "outputId": "4b5a6320-968e-484c-dbe0-293f3eb787e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (type 'exit' to quit): Added python code example for parking lot\n",
            "Response:\n",
            "Sure! Here is an example of a simple parking lot system implemented in Python:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9c5dc2100f19>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your query (type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbzZ1_tTK8Mb",
        "outputId": "1fc24b28-0b08-4229-f1cb-2c8b648c7f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
            "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.5 langchain-core-0.2.9 langchain-text-splitters-0.2.1 langsmith-0.1.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9X0E_iPLFKE",
        "outputId": "0dfbdbb4-cd21-4f9e-d41c-ab0958b10b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install google"
      ],
      "metadata": {
        "id": "ZtJ327-W_17p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "XG6yDjIX59Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the capital of Argentina?'\n",
        "response = model.generate_content(question)\n",
        "response.resolve()\n",
        "response.text\n",
        "chat = model.start_chat(history=[])\n",
        "countries = ['Chile', 'Bolivia']\n",
        "\n",
        "for country in countries:\n",
        "    message = f'What is the capital of {country}?'\n",
        "    chat.send_message(message)\n",
        "\n",
        "chat.history\n",
        "response = chat.send_message('From which countries did I ask you the capital city?')\n",
        "response.resolve()\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UjJmAVeq6AMp",
        "outputId": "8c2a63cb-7081-43b6-f7a7-af77737e12a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-21 03:49:27.798 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 4410.82ms\n",
            "2024-06-21 03:49:29.672 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1871.06ms\n",
            "2024-06-21 03:49:32.714 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3035.74ms\n",
            "2024-06-21 03:49:35.582 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2859.09ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chile and Bolivia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# Transform Gradio history to Gemini format\n",
        "def transform_history(history):\n",
        "    new_history = []\n",
        "    for chat in history:\n",
        "        new_history.append({\"parts\": [{\"text\": chat[0]}], \"role\": \"user\"})\n",
        "        new_history.append({\"parts\": [{\"text\": chat[1]}], \"role\": \"model\"})\n",
        "    return new_history\n",
        "\n",
        "def response(message, history):\n",
        "    global chat\n",
        "    # The history will be the same as in Gradio, the 'Undo' and 'Clear' buttons will work correctly.\n",
        "    chat.history = transform_history(history)\n",
        "    response = chat.send_message(message)\n",
        "    response.resolve()\n",
        "\n",
        "    # Each character of the answer is displayed\n",
        "    for i in range(len(response.text)):\n",
        "        time.sleep(0.05)\n",
        "        yield response.text[: i+1]\n",
        "\n",
        "gr.ChatInterface(response,\n",
        "                 title='Gemini Chat',\n",
        "                 textbox=gr.Textbox(placeholder=\"Question to Gemini\"),\n",
        "                 retry_btn=None).launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LsohKz4P6kIR",
        "outputId": "c77be1cf-3a9a-4526-bf44-f2f85ee7fda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f125722cbf60a65a2e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f125722cbf60a65a2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-21 03:50:42.588 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3221.88ms\n",
            "2024-06-21 03:51:40.612 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5618.42ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 532, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1928, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1526, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 656, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 782, in asyncgen_wrapper\n",
            "    response = await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 584, in _stream_fn\n",
            "    first_response = await async_iteration(generator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 656, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 649, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 632, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"<ipython-input-26-d9f18a5ac2f6>\", line 18, in response\n",
            "    response = chat.send_message(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\", line 505, in send_message\n",
            "    ):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\", line 532, in _check_response\n",
            "    >>> len(chat.history)\n",
            "google.generativeai.types.generation_types.StopCandidateException: finish_reason: OTHER\n",
            "index: 0\n",
            "\n",
            "2024-06-21 03:52:04.362 400 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 685.00ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 532, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1928, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1526, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 656, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 782, in asyncgen_wrapper\n",
            "    response = await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 584, in _stream_fn\n",
            "    first_response = await async_iteration(generator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 656, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 649, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 632, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"<ipython-input-26-d9f18a5ac2f6>\", line 18, in response\n",
            "    response = chat.send_message(message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\", line 496, in send_message\n",
            "    model: The model to use in the chat.\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\", line 262, in generate_content\n",
            "    >>> response = model.generate_content('Tell me a story about a magic backpack', stream=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 812, in generate_content\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 846, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.BadRequest: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: * GenerateContentRequest.contents[3].parts[0].data: required oneof field 'data' must have one initialized field\n",
            "\n",
            "2024-06-21 03:52:34.365 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 7053.16ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f125722cbf60a65a2e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxku2ECE6Vwg",
        "outputId": "86eac1a4-9a1b-4474-fd11-4e2a07322c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.10)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio google-generativeai\n",
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDsR7xbD2vKA",
        "outputId": "052dc5f0-b16b-43c6-c9de-6d5c87283900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.10)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.10)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH-LjCYu10aQ",
        "outputId": "904cfde2-1677-4633-d5ca-4fd3d858b2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install google-generativeai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA9aiBwp1VgA",
        "outputId": "48f6f1c6-2eaf-41a8-db34-d584ae9d2029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.36.0 watchdog-4.0.1\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.1)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.18.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.6.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    }
  ]
}